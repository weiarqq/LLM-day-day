## Tokenizer

### ä½œç”¨:

Tokenizerï¼Œå³å°†è¾“å…¥çš„æ–‡æœ¬åˆ‡åˆ†ä¸ºç‹¬ç«‹çš„æ ‡è®°(token),æ¯ä¸ªtokenæœ‰å¯¹åº”çš„id; éœ€è¦å°†æˆ‘ä»¬çš„æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºæ•°å­—ã€‚

- å°†æ–‡æœ¬åºåˆ—è½¬åŒ–ä¸ºæ•°å­—åºåˆ—(tokenç¼–å·)ï¼Œä½œä¸ºtransformerçš„è¾“å…¥

  [tokenizerå¯è§†åŒ–](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)

### æ¨¡å‹çš„è¾“å…¥:

å¯¹äºæ–‡æœ¬æ¨¡å‹æ¥è¯´ï¼Œè¾“å…¥çš„æ˜¯ä¸€ä¸ªä¸ªæ•°å­—ï¼Œé‚£ä¹ˆå¦‚ä½•å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—å‘¢ï¼Œé‚£å°±æ˜¯tokenizerçš„ä½œç”¨


### åˆ†è¯æ–¹æ¡ˆ:

**åŸºäºå•è¯ï¼ˆWord-basedï¼‰çš„ tokenization**

â€‹	å°†æ–‡æœ¬åˆ’åˆ†ä¸ºä¸€ä¸ªä¸ªè¯(åŒ…æ‹¬æ ‡ç‚¹)

â€‹	ä¼˜ç‚¹ï¼š ç¬¦åˆäººçš„è‡ªç„¶è¯­è¨€å’Œç›´è§‰

â€‹	ç¼ºç‚¹ï¼š 

â€‹		ç›¸åŒå«ä¹‰çš„è¯ä¼šåˆ’åˆ†ä¸ºä¸åŒçš„token,ä¸»è¦åœ¨è‹±æ–‡æ–¹é¢ï¼Œæ¯”å¦‚dogå’Œdogs, å¦å¤–è¯è¡¨ä¼šå¾ˆå¤§ï¼›

â€‹        	ä¸€èˆ¬æ¥è¯´ä¼šé™åˆ¶è¯è¡¨æ•°é‡ï¼Œæœªç™»é™†è¯ç”¨unkownè¡¨ç¤ºï¼Œä½†åŒæ—¶ä¼šå¯¼è‡´éƒ¨åˆ†è¯æ±‡ä¸¢å¤±ä¿¡æ¯ï¼Œå­˜åœ¨OOV(è¾“å…¥çš„è¯è¶…å‡ºè¯è¡¨èŒƒå›´)é—®é¢˜ã€‚


**åŸºäºå­—ç¬¦ï¼ˆCharacter-basedï¼‰çš„ tokenization**

> å°†æ–‡æœ¬åˆ’åˆ†ä¸ºå­—ç¬¦(åŒ…æ‹¬æ ‡ç‚¹)

ä¼˜ç‚¹ï¼š

   å¯ä»¥è¡¨ç¤ºä»»æ„æ–‡æœ¬ï¼Œä¸ä¼šå‡ºç°OOVé—®é¢˜ï¼Œå¯¹æ‹‰ä¸æ–‡è¯­ç³»æ¥è¯´ï¼Œè¯è¡¨å¾ˆå°ï¼Œä¾‹å¦‚è‹±æ–‡åªéœ€è¦256ä¸ªå­—ç¬¦

ç¼ºç‚¹ï¼š

ç›¸å¯¹å•è¯æ¥è¯´ä¿¡æ¯é‡éå¸¸ä½ï¼Œæ¨¡å‹æ€§èƒ½ä¸€èˆ¬å¾ˆå·®

ç›¸å¯¹äº word-based æ¥è¯´ï¼Œä¼šäº§ç”Ÿå¾ˆé•¿çš„ Token åºåˆ—

ä¸­æ–‡ä¹Ÿéœ€è¦å¾ˆå¤§ä¸€ä¸ªè¯è¡¨

åŸºäºå­è¯ï¼ˆsubwordï¼‰çš„ tokenization

> å¸¸ç”¨è¯ä¸åº”è¯¥å†è¢«åˆ‡åˆ†æˆæ›´å°çš„ token æˆ–å­è¯ï¼ˆsubwordï¼‰
>
> ä¸å¸¸ç”¨çš„è¯æˆ–è¯ç¾¤åº”è¯¥ç”¨å­è¯æ¥è¡¨ç¤º

ä½¿ç”¨ subword åˆ’åˆ†ç¬¦åˆè‹±æ–‡è¯ç¾¤ï¼Œèƒ½å¤Ÿå……åˆ†è¡¨æ„



![åˆ†è¯æ–¹æ¡ˆå¯¹æ¯”](./images/LLMä¸­çš„Tokenizers-pdf-08-03-2025_09_18_PM.png  "åˆ†è¯æ–¹æ¡ˆå¯¹æ¯”")

### Subword tokenizers

#### BPE Tokenization

Byte-Pair Encoding(BPE)æœ€åˆæ˜¯ä½œä¸ºä¸€ç§æ–‡æœ¬å‹ç¼©ç®—æ³•å¼€å‘çš„ï¼Œåæ¥è¢« OpenAI ç”¨äº GPT æ¨¡å‹é¢„è®­ç»ƒæ—¶çš„åˆ†è¯ä»»åŠ¡ã€‚è®¸å¤š Transformer æ¨¡å‹éƒ½åœ¨ä½¿ç”¨è¿™ç§ç®—æ³•ï¼ŒåŒ…æ‹¬ GPTã€GPT-2ã€RoBERTaã€BART å’Œ DeBERTaã€‚

https://huggingface.co/learn/llm-course/chapter6/5?fw=pt

##### BPE è®­ç»ƒ

BPE è®­ç»ƒé¦–å…ˆè®¡ç®—è¯­æ–™åº“ä¸­ä½¿ç”¨çš„å”¯ä¸€å•è¯é›†åˆï¼ˆåœ¨å®Œæˆæ ‡å‡†åŒ–å’Œé¢„åˆ†è¯æ­¥éª¤ä¹‹åï¼‰ï¼Œç„¶åå–å‡ºç”¨æ¥ç¼–å†™è¿™äº›è¯çš„æ‰€æœ‰ç¬¦å·æ¥æ„å»ºè¯æ±‡è¡¨ã€‚ä¸¾ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­ï¼Œå‡è®¾æˆ‘ä»¬çš„è¯­æ–™åº“ä½¿ç”¨äº†è¿™äº”ä¸ªè¯ï¼š

```
"hug", "pug", "pun", "bun", "hugs"
```

åŸºç¡€å•è¯é›†åˆå°†æ˜¯ `["b", "g", "h", "n", "p", "s", "u"]` ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒåŸºæœ¬è¯æ±‡è¡¨å°†è‡³å°‘åŒ…å«æ‰€æœ‰ ASCII å­—ç¬¦ï¼Œå¯èƒ½è¿˜åŒ…å«ä¸€äº› Unicode å­—ç¬¦ã€‚å¦‚æœä½ æ­£åœ¨ tokenization ä¸åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­çš„å­—ç¬¦ï¼Œåˆ™è¯¥å­—ç¬¦å°†è½¬æ¢ä¸ºæœªçŸ¥ tokensï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®¸å¤š NLP æ¨¡å‹åœ¨åˆ†æå¸¦æœ‰è¡¨æƒ…ç¬¦å·çš„å†…å®¹çš„ç»“æœéå¸¸ç³Ÿç³•çš„åŸå› ä¹‹ä¸€ã€‚

GPT-2 å’Œ RoBERTa ï¼ˆè¿™ä¸¤è€…éå¸¸ç›¸ä¼¼ï¼‰çš„ tokenizer æœ‰ä¸€ä¸ªå·§å¦™çš„æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼šä»–ä»¬ä¸æŠŠå•è¯çœ‹æˆæ˜¯ç”¨ Unicode å­—ç¬¦ç¼–å†™çš„ï¼Œè€Œæ˜¯ç”¨å­—èŠ‚ç¼–å†™çš„ã€‚è¿™æ ·ï¼ŒåŸºæœ¬è¯æ±‡è¡¨çš„å¤§å°å¾ˆå°ï¼ˆ256ï¼‰ï¼Œä½†æ˜¯èƒ½åŒ…å«å‡ ä¹æ‰€æœ‰ä½ èƒ½æƒ³è±¡çš„å­—ç¬¦ï¼Œè€Œä¸ä¼šæœ€ç»ˆè½¬æ¢ä¸ºæœªçŸ¥ tokens è¿™ä¸ªæŠ€å·§è¢«ç§°ä¸º `å­—èŠ‚çº§ï¼ˆbyte-levelï¼‰ BPE` ã€‚

è·å¾—è¿™ä¸ªåŸºç¡€å•è¯é›†åˆåï¼Œæˆ‘ä»¬é€šè¿‡å­¦ä¹  `åˆå¹¶ï¼ˆmergesï¼‰` æ¥æ·»åŠ æ–°çš„ tokens ç›´åˆ°è¾¾åˆ°æœŸæœ›çš„è¯æ±‡è¡¨å¤§å°ã€‚åˆå¹¶æ˜¯å°†ç°æœ‰è¯æ±‡è¡¨ä¸­çš„ä¸¤ä¸ªå…ƒç´ åˆå¹¶ä¸ºä¸€ä¸ªæ–°å…ƒç´ çš„è§„åˆ™ã€‚æ‰€ä»¥ï¼Œä¸€å¼€å§‹ä¼šåˆ›å»ºå‡ºå«æœ‰ä¸¤ä¸ªå­—ç¬¦çš„ tokens ç„¶åï¼Œéšç€è®­ç»ƒçš„è¿›å±•ï¼Œä¼šäº§ç”Ÿæ›´é•¿çš„å­è¯ã€‚

åœ¨åˆ†è¯å™¨è®­ç»ƒæœŸé—´çš„ä»»ä½•ä¸€æ­¥ï¼ŒBPE ç®—æ³•éƒ½ä¼šæœç´¢æœ€å¸¸è§çš„ç°æœ‰ tokens å¯¹ ï¼ˆåœ¨è¿™é‡Œï¼Œâ€œå¯¹â€æ˜¯æŒ‡ä¸€ä¸ªè¯ä¸­çš„ä¸¤ä¸ªè¿ç»­ tokens ï¼‰ã€‚æœ€å¸¸è§çš„è¿™ä¸€å¯¹ä¼šè¢«åˆå¹¶ï¼Œç„¶åæˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚

å›åˆ°æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ï¼Œè®©æˆ‘ä»¬å‡è®¾å•è¯å…·æœ‰ä»¥ä¸‹é¢‘ç‡ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ„æ€æ˜¯ `"hug"` åœ¨è¯­æ–™åº“ä¸­å‡ºç°äº† 10 æ¬¡ï¼Œ `"pug"` å‡ºç°äº† 5 æ¬¡ï¼Œ `"pun"` å‡ºç°äº† 12 æ¬¡ï¼Œ `"bun"` å‡ºç°äº† 4 æ¬¡ï¼Œ `"hugs"` å‡ºç°äº† 5 æ¬¡ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå­—ç¬¦ï¼ˆå½¢æˆæˆ‘ä»¬åˆå§‹è¯æ±‡è¡¨çš„å­—ç¬¦ï¼‰æ¥å¼€å§‹è®­ç»ƒï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†æ¯ä¸ªå•è¯è§†ä¸ºä¸€ä¸ª tokens åˆ—è¡¨ï¼š

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ç„¶åæˆ‘ä»¬çœ‹çœ‹ç›¸é‚»çš„å­—ç¬¦å¯¹ã€‚ `("h", "u")` åœ¨è¯ `"hug"` å’Œ `"hugs"` ä¸­å‡ºç°ï¼Œæ‰€ä»¥åœ¨è¯­æ–™åº“ä¸­æ€»å…±å‡ºç°äº† 15 æ¬¡ã€‚ç„¶è€Œï¼Œæœ€å¸¸è§çš„å¯¹å±äº `("u", "g")` ï¼Œå®ƒåœ¨ `"hug"` ã€ `"pug"` å’Œ `"hugs"` ä¸­å‡ºç°ï¼Œæ€»å…±åœ¨è¯æ±‡è¡¨ä¸­å‡ºç°äº† 20 æ¬¡ã€‚

å› æ­¤ï¼Œtokenizer å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ `("u", "g") -> "ug"` ï¼Œæ„æ€å°±æ˜¯ `"ug"` å°†è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œä¸”åº”åœ¨è¯­æ–™åº“çš„æ‰€æœ‰è¯ä¸­åˆå¹¶è¿™ä¸€å¯¹ã€‚åœ¨è¿™ä¸ªé˜¶æ®µç»“æŸæ—¶ï¼Œè¯æ±‡è¡¨å’Œè¯­æ–™åº“çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

```
è¯æ±‡è¡¨: ["b", "g", "h", "n", "p", "s", "u", "ug"]
è¯­æ–™åº“: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
```

ç°åœ¨æˆ‘ä»¬æœ‰ä¸€äº›å¯¹ï¼Œç»§ç»­åˆå¹¶çš„è¯ä¼šäº§ç”Ÿä¸€ä¸ªæ¯”ä¸¤ä¸ªå­—ç¬¦é•¿çš„ tokens ä¾‹å¦‚ `("h", "ug")` ï¼Œåœ¨è¯­æ–™åº“ä¸­å‡ºç° 15 æ¬¡ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé˜¶æ®µå‡ºç°é¢‘ç‡æœ€é«˜çš„å¯¹æ˜¯ `("u", "n")` ï¼Œåœ¨è¯­æ–™åº“ä¸­å‡ºç° 16 æ¬¡ï¼Œæ‰€ä»¥å­¦åˆ°çš„ç¬¬äºŒä¸ªåˆå¹¶è§„åˆ™æ˜¯ `("u", "n") -> "un"` ã€‚å°†å…¶æ·»åŠ åˆ°è¯æ±‡è¡¨å¹¶åˆå¹¶æ‰€æœ‰ç°æœ‰çš„è¿™ä¸ªå¯¹ï¼Œå°†å‡ºç°ï¼š

```
è¯æ±‡è¡¨: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
è¯­æ–™åº“: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
```

ç°åœ¨æœ€é¢‘ç¹çš„ä¸€å¯¹æ˜¯ `("h", "ug")` ï¼Œæ‰€ä»¥æˆ‘ä»¬å­¦ä¹ äº†åˆå¹¶è§„åˆ™ `("h", "ug") -> "hug"` ï¼Œè¿™å½¢æˆäº†æˆ‘ä»¬ç¬¬ä¸€ä¸ªä¸‰ä¸ªå­—æ¯çš„ tokens åˆå¹¶åï¼Œè¯­æ–™åº“å¦‚ä¸‹æ‰€ç¤ºï¼š

```
è¯æ±‡è¡¨: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
è¯­æ–™åº“: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
```

æˆ‘ä»¬ç»§ç»­è¿™æ ·åˆå¹¶ï¼Œç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æ‰€éœ€çš„è¯æ±‡é‡ã€‚

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ä½ è®¤ä¸ºä¸‹ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ

##### tokenization

å®Œæˆè®­ç»ƒä¹‹åå°±å¯ä»¥å¯¹æ–°çš„è¾“å…¥ tokenization äº†ï¼Œä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œæ–°çš„è¾“å…¥ä¼šä¾ç…§ä»¥ä¸‹æ­¥éª¤å¯¹æ–°è¾“å…¥è¿›è¡Œ tokenizationï¼š

1. æ ‡å‡†åŒ–
2. é¢„åˆ†è¯
3. å°†å•è¯æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦
4. æ ¹æ®å­¦ä¹ çš„åˆå¹¶è§„åˆ™ï¼ŒæŒ‰é¡ºåºåˆå¹¶æ‹†åˆ†çš„å­—ç¬¦

è®©æˆ‘ä»¬ä»¥æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„ç¤ºä¾‹ä¸ºä¾‹ï¼ŒTokenizer å­¦ä¹ åˆ°äº†ä¸‰ä¸ªåˆå¹¶è§„åˆ™ï¼š

```
("u", "g") -> "ug"
("u", "n") -> "un"
("h", "ug") -> "hug"
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå•è¯ `"bug"` å°†è¢«è½¬åŒ–ä¸º `["b", "ug"]` ã€‚ç„¶è€Œ `"mug"` ï¼Œå°†è¢«è½¬æ¢ä¸º `["[UNK]", "ug"]` ï¼Œå› ä¸ºå­—æ¯ `"m"` ä¸å†åŸºæœ¬è¯æ±‡è¡¨ä¸­ã€‚åŒæ ·ï¼Œå•è¯ `"thug"` ä¼šè¢«è½¬æ¢ä¸º `["[UNK]", "hug"]` ï¼šå­—æ¯ `"t"` ä¸åœ¨åŸºæœ¬è¯æ±‡è¡¨ä¸­ï¼Œä½¿ç”¨åˆå¹¶è§„åˆ™é¦–å…ˆä¼šå°† `"u"` å’Œ `"g"` åˆå¹¶ï¼Œç„¶åå°† `"h"` å’Œ `"ug"` åˆå¹¶ã€‚

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ä½ è®¤ä¸ºè¿™ä¸ªè¯ `"unhug"` å°†å¦‚ä½•è¢« tokenizationï¼Ÿ

##### å®ç° BPE ç®—æ³•

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ BPE ç®—æ³•çš„å®ç°ã€‚è¿™å¹¶ä¸æ˜¯åœ¨å¤§å‹è¯­æ–™åº“ä¸Šå®é™…ä½¿ç”¨çš„ç»è¿‡ä¼˜åŒ–çš„ç‰ˆæœ¬ï¼›æˆ‘ä»¬åªæ˜¯æƒ³å‘ä½ å±•ç¤ºä»£ç ï¼Œä»¥ä¾¿ä½ å¯ä»¥æ›´å¥½åœ°ç†è§£ç®—æ³•

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯­æ–™åº“ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå«æœ‰å‡ å¥è¯çš„ç®€å•è¯­æ–™åº“ï¼š

```
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†è¯¥è¯­æ–™åº“é¢„åˆ†è¯ä¸ºå•è¯ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤ç°ä¸€ä¸ª BPE tokenizer ï¼ˆä¾‹å¦‚ GPT-2ï¼‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `gpt2` åˆ†è¯å™¨è¿›è¡Œé¢„åˆ†è¯ï¼š

```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

ç„¶åï¼Œæˆ‘ä»¬åœ¨è¿›è¡Œé¢„åˆ†è¯çš„åŒæ—¶è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡ï¼š

```
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
defaultdict(int, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1,
    'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1,
    'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1,
    'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
```

ä¸‹ä¸€æ­¥æ˜¯è®¡ç®—åŸºç¡€è¯æ±‡è¡¨ï¼Œè¿™ç”±è¯­æ–™åº“ä¸­ä½¿ç”¨çš„æ‰€æœ‰å­—ç¬¦ç»„æˆï¼š

```
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
  't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
```

æˆ‘ä»¬è¿˜åœ¨è¯¥è¯æ±‡è¡¨çš„å¼€å¤´æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokens å¯¹äº GPT-2ï¼Œå”¯ä¸€çš„ç‰¹æ®Š tokens æ˜¯ `"<|endoftext|>"` ï¼š

```
vocab = ["<|endoftext|>"] + alphabet.copy()
```

æˆ‘ä»¬ç°åœ¨éœ€è¦å°†æ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºå•ç‹¬çš„å­—ç¬¦ï¼Œä»¥ä¾¿èƒ½å¤Ÿå¼€å§‹è®­ç»ƒï¼š

```
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

ç°åœ¨æˆ‘ä»¬å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ¯å¯¹å­—ç¬¦çš„é¢‘ç‡ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒçš„æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨å®ƒï¼š

```
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªå­—å…¸åœ¨åˆå§‹åˆå¹¶åçš„ä¸€äº›ç»“æœï¼š

```
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
```

ç°åœ¨ï¼Œåªéœ€è¦ä¸€ä¸ªç®€å•çš„å¾ªç¯å°±å¯ä»¥æ‰¾åˆ°å‡ºç°é¢‘ç‡æœ€é«˜çš„å¯¹ï¼š

```
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)
('Ä ', 't') 7
```

æ‰€ä»¥ç¬¬ä¸€ä¸ªè¦å­¦ä¹ çš„åˆå¹¶è§„åˆ™æ˜¯ `('Ä ', 't') -> 'Ä t'` ï¼Œæˆ‘ä»¬å°† `'Ä t'` æ·»åŠ åˆ°è¯æ±‡è¡¨ï¼š

```
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æˆ‘ä»¬çš„ `splits` å­—å…¸ä¸­è¿›è¡Œè¿™ä¸ªåˆå¹¶ã€‚è®©æˆ‘ä»¬ä¸ºæ­¤ç¼–å†™å¦ä¸€ä¸ªå‡½æ•°ï¼š

```
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿä¸€ä¸‹ç¬¬ä¸€æ¬¡åˆå¹¶çš„ç»“æœï¼š

```
splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†æˆ‘ä»¬éœ€è¦çš„æ‰€æœ‰ä»£ç ï¼Œå¯ä»¥å¾ªç¯ç›´åˆ°æˆ‘ä»¬å­¦ä¹ åˆ°æˆ‘ä»¬æƒ³è¦çš„æ‰€æœ‰åˆå¹¶ã€‚è®©æˆ‘ä»¬æŠŠç›®æ ‡è¯æ±‡è¡¨çš„å¤§å°è®¾å®šä¸º 50ï¼š

```
vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])
```

æœ€ç»ˆï¼Œæˆ‘ä»¬å­¦ä¹ äº† 19 æ¡åˆå¹¶è§„åˆ™ï¼ˆåˆå§‹è¯æ±‡é‡ä¸º 31 â€”â€” å­—æ¯è¡¨ä¸­çš„ 30 ä¸ªå­—ç¬¦ï¼ŒåŠ ä¸Šç‰¹æ®Š token ï¼‰ï¼š

```
print(merges)
{('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en',
 ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok',
 ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the',
 ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}
```

è¯æ±‡è¡¨ç”±ç‰¹æ®Š token åˆå§‹å­—æ¯å’Œæ‰€æœ‰åˆå¹¶ç»“æœç»„æˆï¼š

```
print(vocab)
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
```

ğŸ’¡ åœ¨åŒä¸€è¯­æ–™åº“ä¸Šä½¿ç”¨ `train_new_from_iterator()` å¯èƒ½ä¸ä¼šäº§ç”Ÿå®Œå…¨ç›¸åŒçš„è¯æ±‡è¡¨ã€‚è¿™æ˜¯å› ä¸ºå½“æœ‰å¤šä¸ªå‡ºç°é¢‘ç‡æœ€é«˜çš„å¯¹æ—¶ï¼Œæˆ‘ä»¬é€‰æ‹©é‡åˆ°çš„ç¬¬ä¸€ä¸ªï¼Œè€Œ ğŸ¤— Tokenizers åº“æ ¹æ®å†…éƒ¨ ID é€‰æ‹©ç¬¬ä¸€ä¸ªã€‚

ä¸ºäº†å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œé¢„åˆ†è¯ã€æ‹†åˆ†ï¼Œç„¶åä½¿ç”¨å­¦åˆ°çš„æ‰€æœ‰åˆå¹¶è§„åˆ™ï¼š

```
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```

æˆ‘ä»¬å¯ä»¥å°è¯•åœ¨ä»»ä½•ç”±å­—æ¯è¡¨ä¸­çš„å­—ç¬¦ç»„æˆçš„æ–‡æœ¬ä¸Šè¿›è¡Œæ­¤æ“ä½œï¼š

```
tokenize("This is not a token.")
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
```







#### Byte-level BPE(BBPE) æ”¹è¿›ç‰ˆæœ¬

å°†å­—èŠ‚(byte) è§†ä¸ºåŸºæœ¬token, BPEæ˜¯å­—ç¬¦çº§åˆ«

ä¸¤ä¸ªå­—èŠ‚åˆå¹¶å³å¯è¡¨ç¤ºUnicodeï¼Œå°†ä¸­æ–‡ã€æ—¥æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ã€è¡¨æƒ…ç¬¦å·ç­‰éƒ½è¿›è¡Œç»Ÿä¸€



#### WordPiece Tokenization

https://huggingface.co/learn/llm-course/zh-CN/chapter6/6?fw=pt

WordPiece æ˜¯ Google å¼€å‘çš„ç”¨äº BERT é¢„è®­ç»ƒçš„åˆ†è¯ç®—æ³•ã€‚è‡ªæ­¤ä¹‹åï¼Œå¾ˆå¤šåŸºäº BERT çš„ Transformer æ¨¡å‹éƒ½å¤ç”¨äº†è¿™ç§æ–¹æ³•ï¼Œæ¯”å¦‚ DistilBERTï¼ŒMobileBERTï¼ŒFunnel Transformers å’Œ MPNETã€‚å®ƒåœ¨è®­ç»ƒæ–¹é¢ä¸ BPE éå¸¸ç±»ä¼¼ï¼Œä½†å®é™…çš„åˆ†è¯æ–¹æ³•æœ‰æ‰€ä¸åŒã€‚

##### WordPiece è®­ç»ƒ

âš ï¸ Google ä»æœªå¼€æº WordPiece è®­ç»ƒç®—æ³•çš„å®ç°ï¼Œå› æ­¤ä»¥ä¸‹æ˜¯æˆ‘ä»¬åŸºäºå·²å‘è¡¨æ–‡çŒ®çš„æœ€ä½³çŒœæµ‹ã€‚å®ƒå¯èƒ½å¹¶é 100ï¼… å‡†ç¡®çš„ã€‚

ä¸BPE ä¸€æ ·ï¼ŒWordPiece ä¹Ÿæ˜¯ä»åŒ…å«æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokens å’Œåˆå§‹å­—æ¯è¡¨çš„å°è¯æ±‡è¡¨å¼€å§‹çš„ã€‚ç”±äºå®ƒæ˜¯é€šè¿‡æ·»åŠ å‰ç¼€ï¼ˆå¦‚ BERT ä¸­çš„ `##` ï¼‰æ¥è¯†åˆ«å­è¯çš„ï¼Œæ¯ä¸ªè¯æœ€åˆéƒ½ä¼šé€šè¿‡åœ¨è¯å†…éƒ¨æ‰€æœ‰å­—ç¬¦å‰æ·»åŠ è¯¥å‰ç¼€è¿›è¡Œåˆ†å‰²ã€‚å› æ­¤ï¼Œä¾‹å¦‚ `"word"` å°†è¢«è¿™æ ·åˆ†å‰²ï¼š

```
w ##o ##r ##d
```

å› æ­¤ï¼Œåˆå§‹å­—æ¯è¡¨åŒ…å«æ‰€æœ‰å‡ºç°åœ¨å•è¯ç¬¬ä¸€ä¸ªä½ç½®çš„å­—ç¬¦ï¼Œä»¥åŠå‡ºç°åœ¨å•è¯å†…éƒ¨å¹¶å¸¦æœ‰ WordPiece å‰ç¼€çš„å­—ç¬¦ã€‚

ç„¶åï¼ŒåŒæ ·åƒ BPE ä¸€æ ·ï¼ŒWordPiece ä¼šå­¦ä¹ åˆå¹¶è§„åˆ™ã€‚ä¸»è¦çš„ä¸åŒä¹‹å¤„åœ¨äºåˆå¹¶å¯¹çš„é€‰æ‹©æ–¹å¼ã€‚WordPiece ä¸æ˜¯é€‰æ‹©é¢‘ç‡æœ€é«˜çš„å¯¹ï¼Œè€Œæ˜¯å¯¹æ¯å¯¹è®¡ç®—ä¸€ä¸ªå¾—åˆ†ï¼Œä½¿ç”¨ä»¥ä¸‹å…¬å¼ï¼š

score=(freq_of_pair)/(freq_of_first_elementÃ—freq_of_second_element)score=(freq_of_pair)/(freq_of_first_elementÃ—freq_of_second_element)

é€šè¿‡å°†ä¸¤éƒ¨åˆ†åˆåœ¨ä¸€èµ·çš„é¢‘ç‡é™¤ä»¥å…¶ä¸­å„éƒ¨åˆ†çš„é¢‘ç‡çš„ä¹˜ç§¯ï¼Œè¯¥ç®—æ³•ä¼˜å…ˆåˆå¹¶é‚£äº›åœ¨è¯æ±‡è¡¨ä¸­å•ç‹¬å‡ºç°å‡ºç°çš„å¯¹ã€‚ä¾‹å¦‚ï¼Œå³ä½¿ `("un", "##able")` è¿™å¯¹åœ¨è¯æ±‡è¡¨ä¸­å‡ºç°çš„é¢‘ç‡å¾ˆé«˜ï¼Œå®ƒä¹Ÿä¸ä¸€å®šä¼šè¢«åˆå¹¶ï¼Œå› ä¸º `"un"` å’Œ `"##able"` è¿™ä¸¤å¯¹å¯èƒ½ä¼šåœ¨å¾ˆå¤šå…¶ä»–è¯ä¸­å‡ºç°ï¼Œé¢‘ç‡å¾ˆé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåƒ `("hu", "##gging")` è¿™æ ·çš„å¯¹å¯èƒ½ä¼šæ›´å¿«åœ°è¢«åˆå¹¶ï¼ˆå‡è®¾å•è¯â€œhuggingâ€åœ¨è¯æ±‡è¡¨ä¸­å‡ºç°çš„é¢‘ç‡å¾ˆé«˜ï¼‰ï¼Œå› ä¸º `"hu"` å’Œ `"##gging"` å¯èƒ½åˆ†åˆ«å‡ºç°çš„é¢‘ç‡è¾ƒä½ã€‚

æˆ‘ä»¬ä½¿ç”¨ä¸ BPE ç¤ºä¾‹ç›¸åŒçš„è¯æ±‡è¡¨ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

ç»è¿‡åˆ†å‰²ä¹‹åå°†ä¼šæ˜¯ï¼š

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

æ‰€ä»¥æœ€åˆçš„è¯æ±‡è¡¨å°†ä¼šæ˜¯ `["b", "h", "p", "##g", "##n", "##s", "##u"]` ï¼ˆå¦‚æœæˆ‘ä»¬æš‚æ—¶å¿½ç•¥ç‰¹æ®Š tokens ï¼‰ã€‚å‡ºç°é¢‘ç‡æœ€é«˜çš„ä¸€å¯¹æ˜¯ `("##u", "##g")` ï¼ˆç›®å‰ 20 æ¬¡ï¼‰ï¼Œä½† `"##u"` å’Œå…¶ä»–å•è¯ä¸€èµ·å‡ºç°çš„é¢‘ç‡éå¸¸é«˜ï¼Œæ‰€ä»¥å®ƒçš„åˆ†æ•°ä¸æ˜¯æœ€é«˜çš„ï¼ˆåˆ†æ•°æ˜¯ 1 / 36ï¼‰ã€‚æ‰€æœ‰å¸¦æœ‰ `"##u"` çš„å¯¹å®é™…ä¸Šéƒ½æœ‰ç›¸åŒçš„åˆ†æ•°ï¼ˆ1 / 36ï¼‰ï¼Œæ‰€ä»¥åˆ†æ•°æœ€é«˜çš„å¯¹æ˜¯ `("##g", "##s")` â€”â€” å”¯ä¸€æ²¡æœ‰ `"##u"` çš„å¯¹â€”â€”åˆ†æ•°æ˜¯ 1 / 20ï¼Œæ‰€ä»¥å­¦ä¹ çš„ç¬¬ä¸€ä¸ªåˆå¹¶æ˜¯ `("##g", "##s") -> ("##gs")` ã€‚

è¯·æ³¨æ„ï¼Œå½“æˆ‘ä»¬åˆå¹¶æ—¶ï¼Œæˆ‘ä»¬ä¼šåˆ é™¤ä¸¤ä¸ª tokens ä¹‹é—´çš„ `##` ï¼Œæ‰€ä»¥æˆ‘ä»¬å°† `"##gs"` æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œå¹¶å°†è¯­æ–™åº“çš„å•è¯æŒ‰ç…§æ”¹è§„åˆ™è¿›è¡Œåˆå¹¶ï¼š

```
è¯æ±‡è¡¨: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
è¯­æ–™åº“: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

æ­¤æ—¶ï¼Œ `"##u"` å‡ºç°åœ¨æ‰€æœ‰å¯èƒ½çš„å¯¹ä¸­ï¼Œå› æ­¤å®ƒä»¬æœ€ç»ˆéƒ½å…·æœ‰ç›¸åŒçš„åˆ†æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¬¬ä¸€ä¸ªå¯¹ä¼šè¢«åˆå¹¶ï¼Œäºæ˜¯æˆ‘ä»¬å¾—åˆ°äº† `("h", "##u") -> "hu"` è§„åˆ™ï¼š

```
è¯æ±‡è¡¨: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
è¯­æ–™åº“: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ç„¶åï¼Œä¸‹ä¸€ä¸ªæœ€ä½³å¾—åˆ†çš„å¯¹æ˜¯ `("hu", "##g")` å’Œ `("hu", "##gs")` ï¼ˆå¾—åˆ†ä¸º 1/15ï¼Œè€Œæ‰€æœ‰å…¶ä»–é…å¯¹çš„å¾—åˆ†ä¸º 1/21ï¼‰ï¼Œå› æ­¤å¾—åˆ†æœ€é«˜çš„ç¬¬ä¸€å¯¹åˆå¹¶ï¼š

```
è¯æ±‡è¡¨: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
è¯­æ–™åº“: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

ç„¶åæˆ‘ä»¬å°±æŒ‰æ­¤æ–¹å¼ç»§ç»­ï¼Œç›´åˆ°æˆ‘ä»¬è¾¾åˆ°æ‰€éœ€çš„è¯æ±‡è¡¨å¤§å°ã€‚

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ä¸‹ä¸€ä¸ªåˆå¹¶è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿ

##### tokenization ç®—æ³•

WordPiece å’Œ BPE çš„åˆ†è¯æ–¹å¼æœ‰æ‰€ä¸åŒï¼ŒWordPiece åªä¿å­˜æœ€ç»ˆè¯æ±‡è¡¨ï¼Œè€Œä¸ä¿å­˜å­¦ä¹ åˆ°çš„åˆå¹¶è§„åˆ™ã€‚WordPiece ä»å¾…åˆ†è¯çš„è¯å¼€å§‹ï¼Œæ‰¾åˆ°è¯æ±‡è¡¨ä¸­æœ€é•¿çš„å­è¯ï¼Œç„¶ååœ¨å…¶å¤„åˆ†å‰²ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸Šè¿°ç¤ºä¾‹ä¸­å­¦ä¹ åˆ°çš„è¯æ±‡è¡¨ï¼Œå¯¹äºè¯ `"hugs"` ï¼Œä»å¼€å§‹å¤„çš„æœ€é•¿å­è¯åœ¨è¯æ±‡è¡¨ä¸­æ˜¯ `"hug"` ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œåˆ†å‰²ï¼Œå¾—åˆ° `["hug", "##s"]` ã€‚ç„¶åæˆ‘ä»¬ç»§ç»­å¤„ç† `"##s"` ï¼Œå®ƒåœ¨è¯æ±‡è¡¨ä¸­ï¼Œæ‰€ä»¥ `"hugs"` çš„åˆ†è¯ç»“æœæ˜¯ `["hug", "##s"]` ã€‚

å¦‚æœä½¿ç”¨ BPEï¼Œæˆ‘ä»¬ä¼šæŒ‰ç…§å­¦ä¹ åˆ°çš„åˆå¹¶è§„åˆ™è¿›è¡Œåˆå¹¶ï¼Œå¹¶å°†å…¶åˆ†è¯ä¸º `["hu", "##gs"]` ï¼Œä¸åŒçš„å­—è¯åˆ†è¯ç®—æ³•æ‰€ä»¥æœ€ç»ˆå¾—åˆ°çš„ç¼–ç æ˜¯ä¸åŒçš„ã€‚

å†ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ `"bugs"` å°†å¦‚ä½•åˆ†è¯çš„ã€‚ `"b"` æ˜¯ä»è¯æ±‡è¡¨ä¸­å•è¯å¼€å¤´å¼€å§‹çš„æœ€é•¿å­è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œåˆ†å‰²å¹¶å¾—åˆ° `["b", "##ugs"]` ã€‚ç„¶å `"##u"` æ˜¯è¯æ±‡è¡¨ä¸­ä» `"##ugs"` å¼€å§‹çš„æœ€é•¿çš„å­è¯ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨é‚£é‡Œæ‹†åˆ†å¹¶å¾—åˆ° `["b", "##u, "##gs"]` ã€‚æœ€åï¼Œ `"##gs"` åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå› æ­¤ `"bugs"` çš„åˆ†è¯ç»“æœæ˜¯ï¼š `["b", "##u, "##gs"]` ã€‚

å½“åˆ†è¯è¿‡ç¨‹ä¸­æ— æ³•åœ¨è¯æ±‡åº“ä¸­æ‰¾åˆ°è¯¥å­è¯æ—¶ï¼Œæ•´ä¸ªè¯ä¼šè¢«æ ‡è®°ä¸º unknownï¼ˆæœªçŸ¥ï¼‰â€”â€” ä¾‹å¦‚ï¼Œ `"mug"` å°†è¢«æ ‡è®°ä¸º `["[UNK]"]` ï¼Œ `"bum"` ä¹Ÿæ˜¯å¦‚æ­¤ï¼ˆå³ä½¿æˆ‘ä»¬çš„è¯æ±‡è¡¨ä¸­åŒ…å« `"b"` å’Œ `"##u"` å¼€å§‹ï¼Œä½†æ˜¯ `"##m"` ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå› æ­¤æœ€ç»ˆçš„åˆ†è¯ç»“æœåªä¼šæ˜¯ `["[UNK]"]` ï¼Œè€Œä¸æ˜¯ `["b", "##u", "[UNK]"]` ï¼‰ã€‚è¿™æ˜¯ä¸ BPE çš„å¦ä¸€ä¸ªåŒºåˆ«ï¼ŒBPE åªä¼šå°†ä¸åœ¨è¯æ±‡åº“ä¸­çš„å•ä¸ªå­—ç¬¦æ ‡è®°ä¸º unknownã€‚

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** `"pugs"` å°†è¢«å¦‚ä½•åˆ†è¯ï¼Ÿ

##### å®ç° WordPiece

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ WordPiece ç®—æ³•çš„å®ç°ã€‚ä¸ BPE ä¸€æ ·ï¼Œè¿™åªæ˜¯æ•™å­¦ç¤ºä¾‹ï¼Œä½ ä¸èƒ½åœ¨å¤§å‹è¯­æ–™åº“ä¸Šä½¿ç”¨ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä¸ BPE ç¤ºä¾‹ä¸­ç›¸åŒçš„è¯­æ–™åº“ï¼š

```
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†è¯­æ–™åº“é¢„åˆ†è¯ä¸ºå•è¯ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤åˆ» WordPiece tokenizer ï¼ˆå¦‚ BERTï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `bert-base-cased` tokenizer è¿›è¡Œé¢„åˆ†è¯ï¼š

```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

ç„¶åæˆ‘ä»¬åœ¨è¿›è¡Œé¢„åˆ†è¯çš„åŒæ—¶ï¼Œè®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡ï¼š

```
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œå­—æ¯è¡¨æ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„é›†åˆï¼Œç”±æ‰€æœ‰å•è¯çš„ç¬¬ä¸€ä¸ªå­—æ¯ä»¥åŠæ‰€æœ‰ä»¥ `##` ä¸ºå‰ç¼€å’Œåœ¨å•è¯ä¸­çš„å…¶ä»–å­—æ¯ç»„æˆï¼š

```
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

æˆ‘ä»¬è¿˜åœ¨è¯¥è¯æ±‡è¡¨çš„å¼€å¤´æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokensï¼Œåœ¨ä½¿ç”¨ BERT çš„æƒ…å†µä¸‹ï¼Œç‰¹æ®Š tokens æ˜¯ `["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]` ï¼š

```
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªå•è¯è¿›è¡Œåˆ†å‰²ï¼Œé™¤äº†ç¬¬ä¸€ä¸ªå­—æ¯å¤–ï¼Œå…¶ä»–å­—æ¯éƒ½éœ€è¦ä»¥ `##` ä¸ºå‰ç¼€ï¼š

```
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è®­ç»ƒäº†ï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ¯å¯¹çš„åˆ†æ•°ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒçš„æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨å®ƒï¼š

```
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹åœ¨åˆå§‹åˆ†å‰²åçš„éƒ¨åˆ†å­—å…¸ï¼š

```
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

ç°åœ¨ï¼Œåªéœ€è¦ä¸€ä¸ªå¿«é€Ÿå¾ªç¯å°±å¯ä»¥æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„å¯¹ï¼š

```
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
('a', '##b') 0.2
```

æ‰€ä»¥ç¬¬ä¸€ä¸ªè¦å­¦ä¹ çš„åˆå¹¶æ˜¯ `('a', '##b') -> 'ab'` ï¼Œå¹¶ä¸”æˆ‘ä»¬æ·»åŠ  `'ab'` åˆ°è¯æ±‡è¡¨ä¸­ï¼š

```
vocab.append("ab")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ `splits` å­—å…¸è¿›è¡Œè¿™ç§åˆå¹¶ã€‚è®©æˆ‘ä»¬ä¸ºæ­¤å†™å¦ä¸€ä¸ªå‡½æ•°ï¼š

```
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ç¬¬ä¸€æ¬¡åˆå¹¶çš„ç»“æœï¼š

```
splits = merge_pair("a", "##b", splits)
splits["about"]
['ab', '##o', '##u', '##t']
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†åˆå¹¶å¾ªç¯çš„æ‰€æœ‰ä»£ç ã€‚è®©æˆ‘ä»¬è®¾å®šè¯æ±‡è¡¨çš„å¤§å°ä¸º 70ï¼š

```
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

ç„¶åæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„è¯æ±‡è¡¨ï¼š

```
print(vocab)
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab','##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œç›¸è¾ƒäº BPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰ï¼Œæ­¤åˆ†è¯å™¨åœ¨å­¦ä¹ å•è¯éƒ¨åˆ†ä½œä¸º tokens æ—¶ç¨å¿«ä¸€äº›ã€‚

ğŸ’¡ åœ¨åŒä¸€è¯­æ–™åº“ä¸Šä½¿ç”¨ `train_new_from_iterator()` ä¸ä¼šäº§ç”Ÿå®Œå…¨ç›¸åŒçš„è¯æ±‡è¡¨ã€‚è¿™æ˜¯å› ä¸º ğŸ¤— Tokenizers åº“æ²¡æœ‰ä¸ºè®­ç»ƒå®ç° WordPieceï¼ˆå› ä¸ºæˆ‘ä»¬ä¸å®Œå…¨ç¡®å®šå®ƒçš„çœŸå®å®ç°æ–¹å¼ï¼‰ï¼Œè€Œæ˜¯ä½¿ç”¨äº† BPEã€‚

è¦å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œæˆ‘ä»¬å…ˆé¢„åˆ†è¯ï¼Œå†è¿›è¡Œåˆ†å‰²ï¼Œç„¶ååœ¨æ¯ä¸ªè¯ä¸Šä½¿ç”¨åˆ†è¯ç®—æ³•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¯»æ‰¾ä»ç¬¬ä¸€ä¸ªè¯å¼€å§‹çš„æœ€å¤§å­è¯å¹¶å°†å…¶åˆ†å‰²ï¼Œç„¶åæˆ‘ä»¬å¯¹ç¬¬äºŒéƒ¨åˆ†é‡å¤æ­¤è¿‡ç¨‹ï¼Œä»¥æ­¤ç±»æ¨ï¼Œå¯¹è¯¥è¯ä»¥åŠæ–‡æœ¬ä¸­çš„åç»­è¯è¿›è¡Œåˆ†å‰²ï¼š

```
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

è®©æˆ‘ä»¬ä½¿ç”¨è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªè¯å’Œä¸€ä¸ªä¸åœ¨è¯æ±‡è¡¨ä¸­çš„è¯ä¸Šæµ‹è¯•ä¸€ä¸‹ï¼š

```
print(encode_word("Hugging"))
print(encode_word("HOgging"))
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªå¯¹æ–‡æœ¬åˆ†è¯çš„å‡½æ•°ï¼š

```
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

æˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•æ–‡æœ¬ä¸Šå°è¯•ï¼š

```
tokenize("This is the Hugging Face course!")
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```



é€šè¿‡å°† pair çš„é¢‘ç‡é™¤ä»¥å…¶å•ä¸ª token çš„é¢‘ç‡çš„ä¹˜ç§¯ï¼Œè¯¥ç®—æ³•ä¼˜å…ˆè€ƒ è™‘å•ä¸ªtoken åœ¨è¯è¡¨ä¸­ä¸å¤ªé¢‘ç¹çš„ pair è¿›è¡Œåˆå¹¶

$pairå¾—åˆ†=\frac{pairå‡ºç°çš„æ¬¡æ•°}{token_1å‡ºç°çš„æ¬¡æ•°\times token2å‡ºç°çš„æ¬¡æ•°}$

ä¾‹å¦‚ï¼Œå®ƒä¸ä¸€å®šä¼šåˆå¹¶ï¼ˆâ€œunâ€ã€â€œ##ableâ€ï¼‰ï¼Œå³ä½¿è¿™å¯¹è¯åœ¨è¯æ±‡è¡¨ä¸­å‡º ç°å¾—å¾ˆé¢‘ç¹ï¼Œå› ä¸ºâ€œunâ€å’Œâ€œ###ableâ€è¿™ä¸¤å¯¹è¯å¯èƒ½éƒ½ä¼šå‡ºç°åœ¨å¾ˆå¤šå…¶ä»–å• è¯ä¸­ï¼Œå¹¶ä¸”é¢‘ç‡å¾ˆé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåƒï¼ˆâ€œhuâ€ã€â€œ##ggingâ€ï¼‰è¿™æ ·çš„ä¸€å¯¹å¯ èƒ½ä¼šæ›´å¿«åœ°èåˆï¼ˆå‡è®¾â€œhuggingâ€ä¸€è¯ç»å¸¸å‡ºç°åœ¨è¯æ±‡è¡¨ä¸­ï¼‰ï¼Œå› ä¸º â€œhuâ€å’Œâ€œ##ggigâ€å•ç‹¬å‡ºç°çš„é¢‘ç‡å¯èƒ½è¾ƒä½ã€‚ è¯·



#### Unigram Tokenization

https://huggingface.co/learn/llm-course/chapter6/7?fw=pt

The Unigram algorithm is used in combination with [SentencePiece](https://huggingface.co/papers/1808.06226), which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet.

Unigram ç®—æ³•å¸¸ç”¨äº SentencePiece ä¸­ï¼Œè¯¥åˆ‡åˆ†ç®—æ³•è¢« AlBERTï¼ŒT5ï¼ŒmBARTï¼ŒBig Bird å’Œ XLNet ç­‰æ¨¡å‹å¹¿æ³›é‡‡ç”¨ã€‚

SentencePiece addresses the fact that not all languages use spaces to separate words. Instead, SentencePiece treats the input as a raw input stream which includes the space in the set of characters to use. Then it can use the Unigram algorithm to construct the appropriate vocabulary.

##### Unigram è®­ç»ƒ

ä¸BPE å’Œ WordPiece ç›¸æ¯”ï¼ŒUnigram çš„å·¥ä½œæ–¹å¼æ­£å¥½ç›¸åï¼šå®ƒä»ä¸€ä¸ªå¤§è¯æ±‡åº“å¼€å§‹ï¼Œç„¶åé€æ­¥åˆ é™¤è¯æ±‡ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¯æ±‡åº“å¤§å°ã€‚æ„å»ºåŸºç¡€è¯æ±‡åº“æœ‰å¤šç§æ–¹æ³•ï¼šä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€‰å–é¢„åˆ‡åˆ†è¯æ±‡ä¸­æœ€å¸¸è§çš„å­ä¸²ï¼Œæˆ–è€…åœ¨å…·æœ‰å¤§è¯æ±‡é‡çš„åˆå§‹è¯­æ–™åº“ä¸Šè¿›è¡Œ BPE å¾—åˆ°ä¸€ä¸ªåˆå§‹è¯åº“ã€‚

åœ¨è®­ç»ƒçš„æ¯ä¸€æ­¥ï¼ŒUnigram ç®—æ³•éƒ½ä¼šåœ¨ç»™å®šå½“å‰è¯æ±‡çš„æƒ…å†µä¸‹è®¡ç®—è¯­æ–™åº“çš„æŸå¤±ã€‚ç„¶åï¼Œå¯¹äºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªç¬¦å·ï¼Œç®—æ³•è®¡ç®—å¦‚æœåˆ é™¤è¯¥ç¬¦å·ï¼Œæ•´ä½“æŸå¤±ä¼šå¢åŠ å¤šå°‘ï¼Œå¹¶å¯»æ‰¾åˆ é™¤åæŸå¤±å¢åŠ æœ€å°‘çš„ç¬¦å·ã€‚è¿™äº›ç¬¦å·å¯¹è¯­æ–™åº“çš„æ•´ä½“æŸå¤±å½±å“è¾ƒå°ï¼Œå› æ­¤ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå®ƒä»¬â€œç›¸å¯¹ä¸å¿…è¦â€å¹¶ä¸”æ˜¯ç§»é™¤çš„æœ€ä½³å€™é€‰è€…ã€‚

è¿™ä¸ªè¿‡ç¨‹éå¸¸æ¶ˆè€—è®¡ç®—èµ„æºï¼Œå› æ­¤æˆ‘ä»¬ä¸åªæ˜¯åˆ é™¤ä¸æœ€ä½æŸå¤±å¢é•¿ç›¸å…³çš„å•ä¸ªç¬¦å·ï¼Œè€Œæ˜¯åˆ é™¤ä¸æœ€ä½æŸå¤±å¢é•¿ç›¸å…³çš„ç™¾åˆ†ä¹‹/p ï¼ˆp æ˜¯ä¸€ä¸ªå¯ä»¥æ§åˆ¶çš„è¶…å‚æ•°ï¼Œé€šå¸¸æ˜¯ 10 æˆ– 20ï¼‰çš„ç¬¦å·ã€‚ç„¶åé‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è¯æ±‡åº“è¾¾åˆ°æ‰€éœ€å¤§å°ã€‚

æ³¨æ„ï¼Œæˆ‘ä»¬æ°¸è¿œä¸ä¼šåˆ é™¤åŸºç¡€çš„å•ä¸ªå­—ç¬¦ï¼Œä»¥ç¡®ä¿ä»»ä½•è¯éƒ½èƒ½è¢«åˆ‡åˆ†ã€‚

ç„¶è€Œï¼Œè¿™ä»ç„¶æœ‰äº›æ¨¡ç³Šï¼šç®—æ³•çš„ä¸»è¦éƒ¨åˆ†æ˜¯åœ¨è¯æ±‡åº“ä¸­è®¡ç®—è¯­æ–™åº“çš„æŸå¤±å¹¶è§‚å¯Ÿå½“æˆ‘ä»¬ä»è¯æ±‡åº“ä¸­ç§»é™¤ä¸€äº›ç¬¦å·æ—¶æŸå¤±å¦‚ä½•å˜åŒ–ï¼Œä½†æˆ‘ä»¬å°šæœªè§£é‡Šå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚è¿™ä¸€æ­¥ä¾èµ–äº Unigram æ¨¡å‹çš„åˆ‡åˆ†ç®—æ³•ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶ã€‚

æˆ‘ä»¬å°†å¤ç”¨å‰é¢ä¾‹å­ä¸­çš„è¯­æ–™åº“ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

è€Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å–è¿™ä¸ªè¯­æ–™åº“ä¸­æ‰€æœ‰çš„å­ä¸²ä½œä¸ºåˆå§‹è¯æ±‡åº“ï¼š

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

##### tokenization ç®—æ³•

Unigram æ¨¡å‹æ˜¯ä¸€ç§è¯­è¨€æ¨¡å‹ï¼Œå®ƒè®¤ä¸ºæ¯ä¸ªç¬¦å·éƒ½ä¸å…¶ä¹‹å‰çš„ç¬¦å·ç‹¬ç«‹ã€‚è¿™æ˜¯æœ€ç®€å•çš„è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤ç»™å®šä¹‹å‰çš„ä¸Šä¸‹æ–‡æƒ…å†µä¸‹ï¼Œç¬¦å· X çš„æ¦‚ç‡å°±æ˜¯ç¬¦å· X çš„æ¦‚ç‡ã€‚æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ Unigram è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬ï¼Œæˆ‘ä»¬çš„é¢„æµ‹æ€»ä¼šè¾“å‡ºæœ€å¸¸è§çš„ç¬¦å·ã€‚

ç»™å®šç¬¦å·çš„æ¦‚ç‡æ˜¯å…¶åœ¨åŸå§‹è¯­æ–™åº“ä¸­çš„é¢‘ç‡ï¼ˆæˆ‘ä»¬è®¡ç®—å®ƒå‡ºç°çš„æ¬¡æ•°ï¼‰ï¼Œé™¤ä»¥è¯æ±‡åº“ä¸­æ‰€æœ‰ç¬¦å·çš„é¢‘ç‡æ€»å’Œï¼ˆä»¥ç¡®ä¿æ¦‚ç‡æ€»å’Œä¸º 1ï¼‰ã€‚ä¾‹å¦‚ï¼Œ `"ug"` å‡ºç°åœ¨ `"hug"` ï¼Œ `"pug"` å’Œ `"hugs"` ä¸­ï¼Œæ‰€ä»¥å®ƒåœ¨æˆ‘ä»¬çš„è¯­æ–™åº“ä¸­çš„é¢‘ç‡æ˜¯ 20ã€‚

ä»¥ä¸‹æ˜¯è¯æ±‡åº“ä¸­æ‰€æœ‰å¯èƒ½å‡ºç°å­è¯çš„é¢‘ç‡ï¼š

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

æ‰€ä»¥ï¼Œæ‰€æœ‰é¢‘ç‡ä¹‹å’Œä¸º 210ï¼Œå­è¯ `"ug"` å‡ºç°çš„æ¦‚ç‡æ˜¯ 20/210ã€‚

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ç¼–å†™ä»£ç è®¡ç®—ä¸Šè¿°é¢‘ç‡ï¼Œç„¶åéªŒè¯ç»“æœçš„å‡†ç¡®æ€§ï¼Œä»¥åŠæ¦‚ç‡çš„æ€»å’Œæ˜¯å¦æ­£ç¡®ã€‚

ç°åœ¨ï¼Œä¸ºäº†å¯¹ä¸€ä¸ªç»™å®šçš„å•è¯è¿›è¡Œåˆ†è¯ï¼Œæˆ‘ä»¬ä¼šæŸ¥çœ‹æ‰€æœ‰å¯èƒ½çš„åˆ†è¯ç»„åˆï¼Œå¹¶æ ¹æ® Unigram æ¨¡å‹è®¡ç®—å‡ºæ¯ç§å¯èƒ½çš„æ¦‚ç‡ã€‚ç”±äºæ‰€æœ‰çš„åˆ†è¯éƒ½è¢«è§†ä¸ºç‹¬ç«‹çš„ï¼Œå› æ­¤è¿™ä¸ªå•è¯åˆ†è¯çš„æ¦‚ç‡å°±æ˜¯æ¯ä¸ªå­è¯æ¦‚ç‡çš„ä¹˜ç§¯ã€‚ä¾‹å¦‚ï¼Œå°† `"pug"` åˆ†è¯ä¸º `["p", "u", "g"]` çš„æ¦‚ç‡ä¸ºï¼š

P([â€˜â€˜p",â€˜â€˜u",â€˜â€˜g"])=P(â€˜â€˜p")Ã—P(â€˜â€˜u")Ã—P(â€˜â€˜g")=5210Ã—36210Ã—20210=0.000389*P*([â€˜â€˜*p*",â€˜â€˜*u*",â€˜â€˜*g*"])=*P*(â€˜â€˜*p*")Ã—*P*(â€˜â€˜*u*")Ã—*P*(â€˜â€˜*g*")=2105Ã—21036Ã—21020=0.000389

ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°† â€œpugâ€ åˆ†è¯ä¸º `["pu", "g"]` çš„æ¦‚ç‡ä¸ºï¼šP([â€˜â€˜pu",â€˜â€˜g"])=P(â€˜â€˜pu")Ã—P(â€˜â€˜g")=5210Ã—20210=0.0022676*P*([â€˜â€˜*p**u*",â€˜â€˜*g*"])=*P*(â€˜â€˜*p**u*")Ã—*P*(â€˜â€˜*g*")=2105Ã—21020=0.0022676

å› æ­¤ï¼Œåè€…çš„å¯èƒ½æ€§æ›´å¤§ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåˆ†è¯æ•°æœ€å°‘çš„åˆ†è¯æ–¹å¼å°†å…·æœ‰æœ€é«˜çš„æ¦‚ç‡ï¼ˆå› ä¸ºæ¯ä¸ªåˆ†è¯éƒ½è¦é™¤ä»¥ 210ï¼‰ï¼Œè¿™æ­£ç¬¦åˆæˆ‘ä»¬çš„ç›´è§‰ï¼šå°†ä¸€ä¸ªè¯åˆ†å‰²ä¸ºå°½å¯èƒ½å°‘çš„å­è¯ã€‚

åˆ©ç”¨ Unigram æ¨¡å‹å¯¹ä¸€ä¸ªè¯è¿›è¡Œåˆ†è¯ï¼Œå°±æ˜¯æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„åˆ†è¯æ–¹å¼ã€‚ä»¥ `"pug"` ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¾—åˆ°çš„å„ç§å¯èƒ½åˆ†è¯æ–¹å¼çš„æ¦‚ç‡å¦‚ä¸‹ï¼š

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

å› æ­¤ï¼Œ `"pug"` å°†è¢«åˆ†è¯ä¸º `["p", "ug"]` æˆ– `["pu", "g"]` ï¼Œå–å†³äºå“ªç§åˆ†è¯æ–¹å¼æ’åœ¨å‰é¢ï¼ˆæ³¨æ„ï¼Œåœ¨æ›´å¤§çš„è¯­æ–™åº“ä¸­ï¼Œåƒè¿™æ ·çš„ç›¸ç­‰æƒ…å†µå°†å¾ˆå°‘è§ï¼‰ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ‰¾å‡ºæ‰€æœ‰å¯èƒ½çš„åˆ†è¯æ–¹å¼å¹¶è®¡ç®—å…¶æ¦‚ç‡æ˜¯å®¹æ˜“çš„ï¼Œä½†åœ¨è¯­æ–™åº“æ¯”è¾ƒå¤§çš„æƒ…å†µä¸‹æœ‰äº›å›°éš¾ã€‚æœ‰ä¸€ä¸ªç»å…¸çš„ç®—æ³•å¯ä»¥ç”¨æ¥è®¡ç®—è¿™ä¸ªæ¦‚ç‡ï¼Œå«åš `Viterbi ç®—æ³•` ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªå›¾æ¥è¡¨ç¤ºä¸€ä¸ªç»™å®šå•è¯çš„æ‰€æœ‰å¯èƒ½åˆ†è¯ï¼Œå¦‚æœä»å­—ç¬¦ `a` åˆ°å­—ç¬¦ `b` çš„å­è¯åœ¨è¯æ±‡è¡¨ä¸­ï¼Œé‚£ä¹ˆå°±å­˜åœ¨ä¸€ä¸ªä» `a` åˆ° `b` çš„åˆ†æ”¯ï¼Œåˆ†æ”¯çš„è¾¹å°±æ˜¯è¿›è¡Œè¿™ä¸ªåˆ‡åˆ†çš„æ¦‚ç‡ã€‚

ä¸ºäº†åœ¨å›¾ä¸­æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„è·¯å¾„ï¼ŒViterbi ç®—æ³•ä¼šç¡®å®šå‡ºæ¯ä¸ªä½ç½®ä¸Šç»“æŸçš„æœ€ä½³å¾—åˆ†åˆ†å‰²ã€‚æˆ‘ä»¬ä»å¤´åˆ°å°¾è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥é€šè¿‡éå†æ‰€æœ‰åœ¨å½“å‰ä½ç½®ç»“æŸçš„å­è¯ï¼Œç„¶åä½¿ç”¨è¿™ä¸ªå­è¯å¼€å§‹ä½ç½®çš„æœ€ä½³å¾—åˆ†ï¼Œæ‰¾åˆ°æœ€é«˜å¾—åˆ†ã€‚ç„¶åï¼Œæˆ‘ä»¬åªéœ€è¦å›æº¯èµ°è¿‡çš„è·¯å¾„ï¼Œå°±èƒ½æ‰¾åˆ°æœ€ç»ˆçš„æœ€ä¼˜è·¯å¾„ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä½¿ç”¨æˆ‘ä»¬çš„è¯æ±‡è¡¨å’Œå•è¯ `"unhug"` çš„ä¾‹å­ã€‚å¯¹äºæ¯ä¸ªä½ç½®ï¼Œæœ€ä½³åˆ‡åˆ†å­è¯çš„åˆ†æ•°å¦‚ä¸‹ï¼š

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

å› æ­¤ â€œunhugâ€ å°†è¢«åˆ†è¯ä¸º `["un", "hug"]` ã€‚

âœï¸ **ç°åœ¨è½®åˆ°ä½ äº†ï¼** ç¡®å®šå•è¯ â€œhuggunâ€ çš„åˆ†è¯æ–¹å¼ä»¥åŠå…¶å¾—åˆ†ã€‚

##### å›åˆ°è®­ç»ƒ

æˆ‘ä»¬å·²ç»äº†è§£äº†å¦‚ä½•è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥æ›´è¯¦ç»†åœ°äº†è§£ä¸€ä¸‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•è®¡ç®—æŸå¤±å€¼ã€‚åœ¨è®­ç»ƒçš„æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬éƒ½ä¼šå°†è¯­æ–™åº“ä¸­çš„æ¯ä¸ªè¯è¿›è¡Œåˆ†è¯ï¼Œåˆ†è¯æ‰€ä½¿ç”¨çš„è¯è¡¨å’Œ Unigram æ¨¡å‹æ˜¯åŸºäºç›®å‰çš„æƒ…å†µï¼ˆå³æ ¹æ®æ¯ä¸ªè¯åœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„é¢‘ç‡ï¼‰æ¥ç¡®å®šçš„ã€‚ç„¶åï¼ŒåŸºäºè¿™ç§åˆ†è¯ç»“æœï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—å‡ºæŸå¤±å€¼ï¼ˆlossï¼‰ã€‚

è¯­æ–™åº“ä¸­çš„æ¯ä¸ªè¯éƒ½æœ‰ä¸€ä¸ªåˆ†æ•°ï¼ŒæŸå¤±ï¼ˆlossï¼‰å€¼æ˜¯è¿™äº›åˆ†æ•°çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ â€”â€” å³æ‰€æœ‰è¯çš„è¯­æ–™åº“ä¸­æ‰€æœ‰è¯çš„ `-log(P(word))` æ€»å’Œ

è®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„ä¾‹å­ï¼Œä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„è¯­æ–™åº“ï¼š

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

æ¯ä¸ªå•è¯çš„åˆ†è¯åŠå…¶ç›¸åº”çš„å¾—åˆ†å¦‚ä¸‹ï¼š

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

å› æ­¤ï¼ŒæŸå¤±å€¼ï¼ˆlossï¼‰æ˜¯ï¼š

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ç§»é™¤æ¯ä¸ª token å¯¹æŸå¤±å€¼çš„å½±å“ã€‚è¿™ä¸ªè¿‡ç¨‹é¢‡ä¸ºç¹çï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œä»…å¯¹ä¸¤ä¸ªå•è¯è¿›è¡Œæ¼”ç¤ºï¼Œåœ¨æˆ‘ä»¬ç¼–å†™ä»£ç æ¥ååŠ©å¤„ç†è¿™ä¸ªè¿‡ç¨‹æ—¶ï¼Œå†å¯¹å…¨éƒ¨çš„è¯è¿›è¡Œ tokenization çš„å¤„ç†ã€‚åœ¨è¿™ä¸ªï¼ˆéå¸¸ï¼‰ç‰¹æ®Šçš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯¹å•è¯çš„ä¸¤ç§ç­‰æ•ˆçš„åˆ†è¯æ–¹å¼ï¼šä¾‹å¦‚ï¼Œâ€œpugâ€å¯ä»¥è¢«åˆ†è¯ä¸º `["pu", "g"]` ï¼Œä¹Ÿå¯ä»¥è¢«åˆ†è¯ä¸º `["p", "ug"]` ï¼Œè·å¾—çš„åˆ†æ•°æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œå»é™¤è¯æ±‡è¡¨ä¸­çš„ `"pu"` æŸå¤±å€¼è¿˜ä¼šæ˜¯ä¸€æ ·çš„ã€‚

ä½†æ˜¯ï¼Œå»é™¤ `"hug"` ä¹‹åï¼ŒæŸå¤±ä¼šå˜å¾—æ›´ç³Ÿï¼Œå› ä¸º `"hug"` å’Œ `"hugs"` çš„ tokenization ä¼šå˜æˆï¼š

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

è¿™äº›å˜åŒ–å°†å¯¼è‡´æŸå¤±å¢åŠ ï¼š

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

å› æ­¤ï¼Œ `"pu"` tokens å¯èƒ½ä¼šä»è¯æ±‡è¡¨ä¸­ç§»é™¤ï¼Œä½† `"hug"` åˆ™ä¸ä¼šã€‚

##### å®ç° Unigram

ç°åœ¨è®©æˆ‘ä»¬åœ¨ä»£ç ä¸­å®ç°ä¸Šé¢çœ‹åˆ°çš„æ‰€å†…å®¹ã€‚ä¸ BPE å’Œ WordPiece ä¸€æ ·ï¼Œè¿™ä¸æ˜¯ Unigram ç®—æ³•çš„é«˜æ•ˆå®ç°ï¼ˆæ°æ°ç›¸åï¼Œè¿™å¥—ä»£ç çš„æ•ˆç‡éå¸¸ä½ï¼‰ï¼Œä½†å®ƒåº”è¯¥å¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£å®ƒã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„è¯­æ–™åº“ä½œä¸ºç¤ºä¾‹ï¼š

```
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

è¿™æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `xlnet-base-cased` ä½œä¸ºæˆ‘ä»¬çš„æ¨¡å‹ï¼š

```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

ä¸ BPE å’Œ WordPiece ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„å‡ºç°æ¬¡æ•°ï¼š

```
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

ç„¶åï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„è¯æ±‡è¡¨åˆå§‹åŒ–ä¸ºå¤§äºæˆ‘ä»¬æœ€ç»ˆæƒ³è¦çš„è¯æ±‡é‡ã€‚æˆ‘ä»¬å¿…é¡»åŒ…å«æ‰€æœ‰åŸºæœ¬çš„å•ä¸ªå­—ç¬¦ï¼ˆå¦åˆ™æˆ‘ä»¬å°†æ— æ³•å¯¹æ¯ä¸ªå•è¯èµ‹äºˆä¸€ä¸ª token ï¼‰ï¼Œä½†å¯¹äºè¾ƒå¤§çš„å­å­—ç¬¦ä¸²ï¼Œæˆ‘ä»¬å°†åªä¿ç•™æœ€å¸¸è§çš„å­—ç¬¦ï¼Œå› æ­¤æˆ‘ä»¬æŒ‰å‡ºç°é¢‘ç‡å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼š

```
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # å¾ªç¯éå†é•¿åº¦è‡³å°‘ä¸º2çš„å­å­—
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# æŒ‰é¢‘ç‡å¯¹å­è¯æ’åº
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
[('â–t', 7), ('is', 5), ('er', 5), ('â–a', 5), ('â–to', 4), ('to', 4), ('en', 4), ('â–T', 3), ('â–Th', 3), ('â–Thi', 3)]
```

æˆ‘ä»¬ç”¨æœ€ä¼˜çš„å­è¯å¯¹å­—ç¬¦è¿›è¡Œåˆ†ç»„ï¼Œä»¥è·å¾—å¤§å°ä¸º 300 çš„åˆå§‹è¯æ±‡è¡¨ï¼š

```
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

ğŸ’¡ SentencePiece ä½¿ç”¨ä¸€ç§åä¸ºå¢å¼ºåç¼€æ•°ç»„ï¼ˆESAï¼‰çš„æ›´é«˜æ•ˆçš„ç®—æ³•æ¥åˆ›å»ºåˆå§‹è¯æ±‡è¡¨ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ‰€æœ‰é¢‘ç‡çš„æ€»å’Œï¼Œå°†é¢‘ç‡è½¬åŒ–ä¸ºæ¦‚ç‡ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å°†å­˜å‚¨æ¦‚ç‡çš„å¯¹æ•°ï¼Œå› ä¸ºç›¸è¾ƒäºå°æ•°ç›¸ä¹˜ï¼Œå¯¹æ•°ç›¸åŠ åœ¨æ•°å€¼ä¸Šæ›´ç¨³å®šï¼Œè€Œä¸”è¿™å°†ç®€åŒ–æ¨¡å‹æŸå¤±çš„è®¡ç®—ï¼š

```
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

ç°åœ¨ï¼Œä¸»å‡½æ•°æ˜¯ä½¿ç”¨ Viterbi ç®—æ³•å¯¹å•è¯è¿›è¡Œåˆ†è¯ã€‚åƒæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„é‚£æ ·ï¼Œè¿™ä¸ªç®—æ³•ä¼šè®¡ç®—å‡ºæ¯ä¸ªè¯çš„æœ€å¥½çš„åˆ†å‰²æ–¹å¼ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªç»“æœä¿å­˜åœ¨ä¸€ä¸ªå«åš `best_segmentations` çš„å˜é‡é‡Œã€‚æˆ‘ä»¬ä¼šä¸ºè¯çš„æ¯ä¸€ä¸ªä½ç½®ï¼ˆä» 0 å¼€å§‹ï¼Œä¸€ç›´åˆ°è¯çš„æ€»é•¿åº¦ï¼‰éƒ½ä¿å­˜ä¸€ä¸ªå­—å…¸ï¼Œå­—å…¸é‡Œæœ‰ä¸¤ä¸ªé”®ï¼šæœ€å¥½çš„åˆ†å‰²ä¸­æœ€åä¸€ä¸ªè¯çš„èµ·å§‹ä½ç½®ï¼Œä»¥åŠæœ€å¥½çš„åˆ†å‰²çš„å¾—åˆ†ã€‚æœ‰äº†æœ€åä¸€ä¸ªè¯çš„èµ·å§‹ä½ç½®ï¼Œå½“æˆ‘ä»¬æŠŠæ•´ä¸ªåˆ—è¡¨éƒ½å¡«æ»¡åï¼Œæˆ‘ä»¬å°±èƒ½æ‰¾åˆ°å®Œæ•´çš„åˆ†å‰²æ–¹å¼ã€‚

æˆ‘ä»¬åªéœ€è¦ä¸¤ä¸ªå¾ªç¯å°±å¯ä»¥å¡«å……è¿™ä¸ªåˆ—è¡¨ï¼šä¸€ä¸ªä¸»å¾ªç¯ç”¨æ¥éå†æ¯ä¸ªå¯èƒ½çš„å¼€å§‹ä½ç½®ï¼Œç¬¬äºŒä¸ªå¾ªç¯åˆ™è¯•ç€æ‰¾å‡ºæ‰€æœ‰ä»¥è¿™ä¸ªå¼€å§‹ä½ç½®å¼€å§‹çš„å­ä¸²ã€‚å¦‚æœè¿™ä¸ªå­ä¸²åœ¨æˆ‘ä»¬çš„è¯è¡¨é‡Œï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æ‰¾åˆ°äº†ä¸€ä¸ªæ–°çš„åˆ†è¯æ–¹å¼ï¼Œè¿™ä¸ªåˆ†è¯æ–¹å¼ä¼šåœ¨å½“å‰ä½ç½®ç»“æŸã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼šæŠŠè¿™ä¸ªæ–°çš„åˆ†è¯æ–¹å¼å’Œ `best_segmentations` é‡Œçš„å†…å®¹è¿›è¡Œæ¯”è¾ƒã€‚

å½“ä¸»å¾ªç¯ç»“æŸåï¼Œæˆ‘ä»¬å°±ä»è¯çš„æœ€åä¸€ä¸ªä½ç½®å¼€å§‹ï¼Œç„¶åä¸€æ­¥æ­¥å¾€å‰è·³ï¼Œè·³è¿‡çš„æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬éƒ½ä¼šè®°å½•ä¸‹æ¥ï¼Œç›´åˆ°æˆ‘ä»¬å›åˆ°è¯çš„å¼€å¤´ã€‚

```
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # best_score_at_startåº”è¯¥ç”±å¾ªç¯çš„å‰é¢çš„æ­¥éª¤è®¡ç®—å’Œå¡«å……
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # å¦‚æœæˆ‘ä»¬å‘ç°ä»¥ end_idx ç»“å°¾çš„æ›´å¥½åˆ†æ®µ,æˆ‘ä»¬ä¼šæ›´æ–°
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # æˆ‘ä»¬æ²¡æœ‰æ‰¾åˆ°å•è¯çš„ tokens  -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

æˆ‘ä»¬å·²ç»å¯ä»¥åœ¨ä¸€äº›è¯ä¸Šå°è¯•æˆ‘ä»¬çš„åˆå§‹æ¨¡å‹ï¼š

```
print(encode_word("Hopefully", model))
print(encode_word("This", model))
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

ç°åœ¨ï¼Œè®¡ç®—è¯­æ–™åº“ä¸Šçš„åˆ†è¯æŸå¤±å°±å¾ˆç®€å•äº†ï¼

```
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦æœ‰æ•ˆï¼š

```
compute_loss(model)
413.10377642940875
```

è®¡ç®—æ¯ä¸ªè¯çš„åˆ†æ•°ä¹Ÿå¹¶ééš¾äº‹ï¼›æˆ‘ä»¬åªéœ€è¦è®¡ç®—é€šè¿‡åˆ é™¤æ¯ä¸ªè¯å¾—åˆ°çš„æ¨¡å‹çš„æŸå¤±ï¼š

```
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # æˆ‘ä»¬å°†ä¿ç•™é•¿åº¦ä¸º 1 çš„ tokens
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

æˆ‘ä»¬å¯ä»¥è¯•è¯•çœ‹å¯¹äºç»™å®šçš„è¯æ˜¯å¦æœ‰æ•ˆï¼š

```
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

å› ä¸º `"ll"` è¿™ä¸ªå­è¯åœ¨ `"Hopefully"` è¿™ä¸ªè¯çš„åˆ†è¯ä¸­è¢«ä½¿ç”¨äº†ï¼Œå¦‚æœæˆ‘ä»¬æŠŠå®ƒåˆ æ‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šéœ€è¦ç”¨ä¸¤ä¸ª `"l"` æ¥ä»£æ›¿ï¼Œæ‰€ä»¥æˆ‘ä»¬é¢„è®¡å®ƒä¼šå¯¼è‡´æŸå¤±å€¼å¢åŠ ã€‚è€Œ `"his"` è¿™ä¸ªè¯åªåœ¨ `"This"` è¿™ä¸ªè¯é‡Œé¢è¢«ä½¿ç”¨ï¼Œè€Œä¸” `"This"` æ˜¯ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„è¯è¢«åˆ†å‰²çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬é¢„è®¡åˆ é™¤å®ƒçš„æŸå¤±å€¼å˜åŒ–ä¼šæ˜¯é›¶ã€‚ä¸‹é¢å°±æ˜¯å®éªŒç»“æœï¼š

```
6.376412403623874
0.0
```

ğŸ’¡ è¿™ç§æ–¹å¼æ•ˆç‡éå¸¸ä½ï¼Œæ‰€ä»¥ SentencePiece ä½¿ç”¨äº†ä¸€ç§ä¼°ç®—æ–¹æ³•æ¥è®¡ç®—å¦‚æœæ²¡æœ‰ X tokenï¼Œæ¨¡å‹çš„æŸå¤±ä¼šæ˜¯å¤šå°‘ï¼šå®ƒä¸æ˜¯é‡æ–°å¼€å§‹ï¼Œè€Œæ˜¯åªæ˜¯ç”¨å‰©ä¸‹çš„è¯è¡¨é‡Œ X token çš„åˆ†è¯æ–¹å¼æ¥æ›¿ä»£å®ƒã€‚è¿™æ ·ï¼Œæ‰€æœ‰çš„å¾—åˆ†éƒ½èƒ½åœ¨å’Œæ¨¡å‹æŸå¤±ä¸€èµ·çš„åŒæ—¶è®¡ç®—å‡ºæ¥ã€‚

è‡³æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åšçš„æœ€åä¸€ä»¶äº‹å°±æ˜¯å°†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokens æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œç„¶åå¾ªç¯ç›´åˆ°æˆ‘ä»¬ä»è¯æ±‡è¡¨ä¸­å‰ªé™¤è¶³å¤Ÿå¤šçš„ tokens ä»¥è¾¾åˆ°æˆ‘ä»¬æœŸæœ›çš„è§„æ¨¡ï¼š

```
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # åˆ é™¤åˆ†æ•°æœ€ä½çš„percent_to_remov tokens ã€‚
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

ç„¶åï¼Œè¦å¯¹æŸäº›æ–‡æœ¬è¿›è¡Œ tokenizationï¼Œæˆ‘ä»¬åªéœ€è¿›è¡Œé¢„åˆ†è¯ç„¶åä½¿ç”¨æˆ‘ä»¬çš„ `encode_word()` å‡½æ•°ï¼š

```
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
['â–This', 'â–is', 'â–the', 'â–Hugging', 'â–Face', 'â–', 'c', 'ou', 'r', 's', 'e', '.']
```

è‡³æ­¤ Unigram çš„ä»‹ç»å®Œæ¯•ï¼æœŸæœ›æ­¤åˆ»ä½ å·²æ„Ÿè§‰è‡ªèº«å¦‚åŒé¢†åŸŸçš„ä¸“å®¶ä¸€èˆ¬ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ğŸ¤—Tokenizers åº“çš„åŸºæœ¬æ„é€ æ¨¡å—ï¼Œå¹¶å±•ç¤ºå¦‚ä½•ä½¿ç”¨å®ƒä»¬æ„å»ºè‡ªå·±çš„ tokenizer



Chat_template

```jinja2
{# Initialize default for add_generation_prompt if undefined #}
{% if not add_generation_prompt is defined %}
  {% set add_generation_prompt = false %}
{% endif %}

{# Namespace for state tracking variables #}
{% set ns = namespace(
    is_first=false,
    is_tool=false,
    is_output_first=true,
    system_prompt='',
    is_first_sp=true
) %}

{# Process system messages first #}
{%- for message in messages %}
  {%- if message['role'] == 'system' %}
    {%- if ns.is_first_sp %}
      {% set ns.system_prompt = ns.system_prompt + message['content'] %}
      {% set ns.is_first_sp = false %}
    {%- else %}
      {% set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}
    {%- endif %}
  {%- endif %}
{%- endfor %}

{# Output beginning and system content #}
{{ bos_token }}{{ ns.system_prompt }}

{# Process conversation messages #}
{%- for message in messages %}
  {# USER MESSAGE #}
  {%- if message['role'] == 'user' %}
    {%- set ns.is_tool = false -%}
    '' + {{ message['content'] }}

  {# ASSISTANT TOOL CALLS #}
  {%- elif message['role'] == 'assistant' and message['content'] is none %}
    {%- set ns.is_tool = false -%}
    {%- for tool in message['tool_calls'] %}
      {%- if not ns.is_first %}
        'â–' + {{ tool['type'] }} + 'â–š' + {{ tool['function']['name'] }} + '\n```json\n' + {{ tool['function']['arguments'] }} + '\n```' + ' â–'
        {%- set ns.is_first = true -%}
      {%- else %}
        '\nâ–' + {{ tool['type'] }} + 'â–š' + {{ tool['function']['name'] }} + '\n```json\n' + {{ tool['function']['arguments'] }} + '\n```' + ' â–'
        'â–Œ'
      {%- endif %}
    {%- endfor %}

  {# ASSISTANT TEXT RESPONSE #}
  {%- elif message['role'] == 'assistant' and message['content'] is not none %}
    {%- if ns.is_tool %}
      'â–Œ' + {{ message['content'] }} + ''
    {%- else %}
      'â–Œ' + {{ message['content'] }} + ''
    {%- endif %}
    {%- set ns.is_tool = false -%}

  {# TOOL OUTPUT #}
  {%- elif message['role'] == 'tool' %}
    {%- set ns.is_tool = true -%}
    {%- if ns.is_output_first %}
      ' â–šâ–š' + {{ message['content'] }} + ' â–'
      {%- set ns.is_output_first = false %}
    {%- else %}
      '\nâ–' + {{ message['content'] }} + ' â–'
    {%- endif %}
  {%- endif %}
{%- endfor %}

{# Trailing generation prompts #}
{% if ns.is_tool %}
  'â–Œ'
{% endif %}
{% if add_generation_prompt and not ns.is_tool %}
  'â–Œ'
{% endif %}
```







https://www.bilibili.com/video/BV1Fc411C7sz/?spm_id_from=333.337.search-card.all.click
