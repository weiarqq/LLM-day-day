|系列| 模型 | Report | Models | GitHub | 发布时间 |
|----| :--- | ----- |:---|----|----|
|Llama| Llama | https://arxiv.org/abs/2302.13971 |  |  | 2023-02 |
| Llama | Llama 2     | https://arxiv.org/abs/2307.09288                             | [meta-llama/Llama-2-70b-hf](https://huggingface.co/meta-llama/Llama-2-70b-hf) |  | 2023-07 |
| Qwen        | qwen        | https://arxiv.org/abs/2309.16609                             | [Qwen/Qwen-72B](https://huggingface.co/Qwen/Qwen-72B) | https://github.com/QwenLM/Qwen | 2023-09 |
| deepseek    | deepseek    | https://arxiv.org/abs/2401.02954                             | [deepseek-ai/deepseek-llm-67b-base](https://huggingface.co/deepseek-ai/deepseek-llm-67b-base) | https://github.com/deepseek-ai/DeepSeek-LLM | 2024-01 |
| deepseek | deepseek-v2 | https://arxiv.org/abs/2405.04434                             | [deepseek-ai/DeepSeek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2) | https://github.com/deepseek-ai/DeepSeek-V2  | 2024-06 |
| Llama | Llama 3     | https://arxiv.org/abs/2407.21783                             | [meta-llama/Meta-Llama-3-70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B) |  | 2024-07 |
| Qwen | qwen2       | https://arxiv.org/abs/2407.10671                             | [Qwen/Qwen2-72B](https://huggingface.co/Qwen/Qwen2-72B) | https://qwenlm.github.io/blog/qwen3/        | 2024-07 |
| OLMo    | OLMo 2 | https://arxiv.org/abs/2501.00656 | [allenai/OLMo-2-1124-13B](https://huggingface.co/allenai/OLMo-2-1124-13B) | https://github.com/allenai/OLMo | 2024-12 |
|Qwen|qwen2.5|https://arxiv.org/abs/2412.15115 |[Qwen/Qwen2.5-72B](https://huggingface.co/Qwen/Qwen2.5-72B)||2024-12|
| deepseek | deepseek-v3 | https://arxiv.org/abs/2412.19437                     | [deepseek-ai/DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) | https://github.com/deepseek-ai/DeepSeek-V3  | 2024-12 |
| deepseek | deepseek-r1 | https://arxiv.org/abs/2501.12948 | [deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) | https://github.com/deepseek-ai/DeepSeek-R1  | 2025-01 |
| Google      | Gemma 3     | https://arxiv.org/abs/2503.19786 | [google/gemma-3-27b-pt](https://huggingface.co/google/gemma-3-27b-pt) | https://github.com/gemma-3/gemma-3 | 2025-03 |
| Mistral | Mistral Small 3.1 | https://mistral.ai/news/mistral-small-3-1 | [mistralai/Mistral-Small-3.1-24B-Base-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503) |                                             | 2025-03 |
| Llama | Llama 4     | https://ai.meta.com/blog/llama-4-multimodal-intelligence/ | [meta-llama/Llama-4-Maverick-17B-128E](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E) |  | 2025-04 |
| Qwen | qwen3       | https://arxiv.org/abs/2505.09388                             | [Qwen/Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B) | https://github.com/QwenLM/Qwen3             | 2025-05 |
|SmolLM3| SmolLM3 | https://huggingface.co/blog/smollm3 | [HuggingFaceTB/SmolLM3-3B](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) |  | 2025-07 |
| kimi        | kimi k2     | https://arxiv.org/abs/2507.20534                             | [moonshotai/Kimi-K2-Base](https://huggingface.co/moonshotai/Kimi-K2-Base) | https://github.com/MoonshotAI/Kimi-K2 | 2025-07 |
|GLM| GLM-4.5 | https://arxiv.org/abs/2508.06471 | [zai-org/GLM-4.5](https://huggingface.co/zai-org/GLM-4.5) | https://github.com/zai-org/GLM-4.5?tab=readme-ov-file | 2025-08 |
| GPT         | gpt-oss     | https://openai.com/zh-Hans-CN/index/introducing-gpt-oss/ | [openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) | https://github.com/openai/gpt-oss | 2025-08 |







| 模型                    | deepseek-v3 | qwen3 | kimi-k2 | gpt-oss | llama-4 |
| ----------------------- | ----------- | ----- | ------- | ------- | ------- |
| 模型大小              | 671B/37B | 235B/22B | 1T/32B | 120B/5.1B | 400B/17B |
| vocab_size              | 129K | 151K | 160k | 200K | 202K |
| rope_type               | yarn | default | yarn | yarn |         |
| rope_beta               | 10000 | 1000000 | 50000 | 150000 |         |
| max_seq_length          | 128K | 128K | 128k | 128k | 512K |
| num_hidden_layers       | 61 | 94 | 61 | 36 | 48 |
| attention_type          | MLA | GQA | MLA | GQA |         |
| head_num/分组数 | \ | 64/4 | \ | 64/8 |         |
| MOE           | 3密集+其他MOE | 全MOE | 1密集+其他MOE | \ |  |
| 专家/共享专家/激活专家  | 256/1/8+1 | 128/0/8 | 384/1/8+1 | 128/0/4 |         |
| normalization           | RMSNorm | RMSNorm | RMSNorm | RMSNorm |         |
| normalization位置       | Pre-Norm | Pre-Norm | Pre-Norm | Pre-Norm |         |
|                         |             |       |         |         |         |
|                         |             |       |         |         |         |
|                         |             |       |         |         |         |





![img](/Users/wangqi/workspace/document/LLM-learn/images/model_structure.webp)





**Llama**

Tokenization: 使用BPE算法进行分词，数字拆分为单个数字

normalization: Pre-Norm 前置归一化，使用RMSNorm进行归一化，为了提高训练稳定性，并提升计算速度，对每个Transformer子层的输入进行归一化，而不是对输出进行归一化。

激活函数：SwiGLU激活函数替代了ReLU非线性函数，以提升性能，使用的维度是\($\frac{8}{3} d$\)，而非PaLM中使用的$4 d$。

位置编码: 移除了绝对位置嵌入，转而在模型的每一层添加了旋转位置嵌入(RoPE).

Optimizer：AdamW优化器,采用以下超参数：$\beta_{1}=0.9,\ \beta_{2}=0.95$。使用cosine学习率调度，最终学习率等于最大学习率的10%。使用0.1的权重衰减和1.0的梯度裁剪，预热2000步，并根据模型大小调整学习率和批量大小。

**Llama2**

上下文长度: 从2048提高到4096

Grouped-Query-Attention: 分组注意力机制，减小KV-Cache的缓存占用，多个head之间共享K,V的投影。K(hidden_states), V(hidden_states)  相对于 Q(hidden_states) head数量减少，再进行复制。

**Qwen** 

Tokenization: 扩大词表，词汇表中增加了常用汉字、词语以及其他语言的常用词汇，数字拆分为单个数字，词汇表规模约为15.2万。

输入输出层不共享参数：即 Linear(vocab, hidden_size)不进行共享，能提升模型性能。其他模型共享了？？？？

偏置：对于大多数层，移除偏置，但在注意力机制的QKV层中添加偏置，以增强模型的外推能力。

上下文扩展：使用NTK-aware interpolation实现无需训练的上下文外推能力。

> 与位置插值（PI）（Chen等人，2023a）对RoPE的每个维度进行同等缩放不同，NTK感知插值通过调整RoPE的基数，以无需训练的方式防止高频信息丢失。为了进一步提升性能，我们还实现了一个简单的扩展，称为动态NTK感知插值，这一点在（Peng等人，2023a）中会有更正式的讨论。它按块动态改变缩放比例，避免性能严重下降。这些技术使我们能够有效扩展Transformer模型的上下文长度，同时不影响其计算效率或准确性。
>
> QWEN 还融入了两种注意力机制：LogN-Scaling和窗口注意力（Beltagy 等人，2020）。LogN-Scaling通过一个取决于上下文长度与训练长度之比的因子，对查询和值的点积进行重新缩放，确保随着上下文长度的增加，注意力值的熵保持稳定。窗口注意力将注意力限制在有限的上下文窗口内，防止模型关注距离过远的标记。
>
> 我们还观察到，我们模型的长上下文建模能力在不同层之间存在差异，与较高层相比，较低层在上下文长度扩展方面更为敏感。为了利用这一观察结果，我们为每一层分配不同的窗口大小，较低层使用较短的窗口，较高层使用较长的窗口。



**Deepseek** 

模型参数：使用GQA时，加深网络层数，而非加宽FFN宽度的做法来增加模型参数。GQA采用分组共享参数，则会减少可训练参数。

Optimizer： 预训练过程中采用了多步学习率调度器，而非典型的余弦调度器。具体来说，模型的学习率在经过2000个预热步骤后达到最大值，然后在处理完80%的训练token后降至最大值的31.6%。在处理完90%的token后，学习率进一步降至最大值的10%。训练阶段的梯度裁剪设置为1.0。

> 从训练趋势来看，如图1（a）所示，使用多步学习率调度器的最终性能与余弦调度器基本一致。在保持模型大小不变的情况下调整训练规模时，多步学习率调度器允许复用第一阶段的训练成果，这为持续训练提供了独特的便利。因此，我们选择多步学习率调度器作为默认设置。我们还在图1（b）中证明，调整多步学习率调度器中不同阶段的比例可以带来稍好的性能。不过，为了平衡持续训练中的复用率和模型性能，我们选择了上述三个阶段分别为80%、10%和10%的分布比例。

Scaling laws: 讨论了缩放定律问题

> • 我们确立了超参数的缩放定律，为确定最佳超参数提供了一个经验框架。
>
> 我们没有采用模型参数N，而是采用非嵌入FLOPs/token $M$来表示模型规模，这使得大规模模型的最优模型/数据扩展分配策略更加准确，对泛化损失的预测也更好。
>
> 预训练数据的质量影响最佳模型/数据扩展分配策略。数据质量越高，分配给模型扩展的计算预算就越多。

**OLMo** 

偏置项: 无偏置项,借鉴LLaMA、PaLM等模型的做法，剔除了所有偏置项，以提高训练稳定性。

normalization: 归一化层不添加可训练参数。使用层归一化的非参数公式，在归一化内没有仿射变换，即没有“bias”（偏置）认为是最安全的选择，并且速度快。

**OLMo 2**

QK-Norm: QK归一化,在计算attention之前使用RMSNorm对 $Key$ 和$Query$的投影进行归一化。**避免注意力对数过大，过大的注意力对数可能导致训练损失发散**。即先计算 K(hidden_states)、Q(hidden_states), 然后RMSNorm，然后再添加ROPE,再计算score。

Normalization: 退回使用了RMSNorm, 但使用了后归一化(Post-Norm)进行重排序归一化：对每个Transformer块内注意力层和前馈（MLP）层的输出进行归一化，而非输入。因此，每个块的公式变为： 
$$
h:=x+RMSNorm(Attention(x)) \\
h_{out}:=h+RMSNorm(MLP(x))
$$
其中，$x$是层的输入，$h$是中间隐藏状态，$h_{out}$是输出。目的为提升训练稳定性。先计算再归一化再残差。

> 研究表明，后归一化有助于训练稳定性，尤其是在不使用精心设计的学习率预热策略时。OLMo 2的训练损失曲线表明，这种设计在训练过程中表现更为稳定。 

transformer原始归一化位置、Llama 3归一化位置、OLMo 2归一化位置对比

![img](/Users/wangqi/workspace/document/LLM-learn/images/norm.png)

Z-loss: 采用Z损失正则化，经验表明它能提高运行稳定性。

位置编码: RoPE的$θ$从10,000增加到500,000。这种方法提高了位置编码的分辨率。

**deepseek-v2**

MOE架构:

DeepSeekMoE有两个核心理念：将专家分割为更精细的粒度，以实现更高的专家专业性和更准确的知识获取；隔离一些共享专家，以减少路由专家之间的知识冗余。在激活专家参数和总专家参数数量相同的情况下，DeepSeekMoE能够大幅优于GShard（Lepikhin等人，2021）等传统的混合专家（MoE）架构。

设$u_{t}$为第$t$个标记的FFN输入，我们按如下方式计算FFN输出$h_{t}'$：
$$
h_{t}'=u_{t}+\sum_{i=1}^{N_{s}} FFN_{i}^{(s)}\left(u_{t}\right)+\sum_{i=1}^{N_{r}} g_{i, t} FFN_{i}^{(r)}\left(u_{t}\right), \quad(20)\\
g_{i, t}= \begin{cases}s_{i, t}, & s_{i, t} \in Topk\left(\left\{s_{j, t} | 1 \leq j \leq N_{r}\right\}, K_{r}\right), \\ 0, & otherwise, \end{cases} \quad(21) \\
s_{i, t}=Softmax_{i}\left(u_{t}^{T} e_{i}\right), \quad(22) 
$$
其中，$N_{5}$和$N_{r}$分别表示共享专家和路由专家的数量；$FFN_{i}^{(s)}(\cdot)$和$FFN_{i}^{(r)}(\cdot)$分别表示第$i$个共享专家和第$i$个路由专家；$K_{1}$表示激活的路由专家数量；$g_{i, t}$是第$i$个专家的门控值；$s_{i, t}$是标记-专家亲和力；$e_{i}$是该层中第$i$个路由专家的质心；$Topk (\cdot, K)$表示由第$t$个标记与所有路由专家计算出的亲和力分数中$K$个最高分数组成的集合。

2.2.2. 设备受限路由

我们设计了一种受设备限制的路由机制，以限制与MoE相关的通信成本。当采用专家并行时，被路由的专家将分布在多个设备上。对于每个令牌，其与MoE相关的通信频率与目标专家所覆盖的设备数量成正比。由于DeepSeekMoE中专家分割的粒度较细，激活的专家数量可能较多，因此如果采用专家并行，与MoE相关的通信成本将会更高。

对于DeepSeek-V2，除了简单地对路由专家进行top-K选择外，我们还确保每个令牌的目标专家最多分布在M个设备上。具体来说，对于每个令牌，我们首先选择M个设备，这些设备上的专家具有最高的亲和度分数。然后，我们在这M个设备上的专家中进行top-K选择。在实践中，我们发现当\(M ≥3\)时，受设备限制的路由能够取得与无限制top-K路由大致相当的良好性能。

2.2.3. 负载均衡的辅助损失

我们在自动学习的路由策略中考虑了负载均衡。首先，不平衡的负载会增加路由崩溃的风险（Shazeer 等人，2017），导致部分专家无法得到充分训练和利用。其次，在采用专家并行时，不平衡的负载会降低计算效率。在 DeepSeek-V2 的训练过程中，我们设计了三种辅助损失，分别用于控制专家级负载均衡$(\mathcal{L}_ExpBal)$、设备级负载均衡$(\mathcal{L}_{DevBal })$和通信均衡$(\mathcal{L}_{CommBal })$。

专家级平衡损失。我们使用专家级平衡损失（Fedus等人，2021；Lepikhin等人，2021）来降低路由崩溃的风险： 
$$
\mathcal{L}_{ExpBal }=\alpha_{1} \sum_{i=1}^{N_{r}} f_{i} P_{i}, \quad(23)\\  f_{i}=\frac{N_{r}}{K_{r} T} \sum_{t=1}^{T} \mathbb{1}( Token t selects Expert i), \quad(24)\\
P_{i}=\frac{1}{T} \sum_{t=1}^{T} s_{i, t}, \quad(25) 
$$
其中，$\alpha_{1}$是一个被称为专家级平衡因子的超参数；1表示指示函数；T表示序列中的标记数量。

设备级平衡损失。除了专家级平衡损失外，我们还设计了一种设备级平衡损失，以确保不同设备间的计算平衡。在DeepSeek-V2的训练过程中，我们将所有路由专家划分为D组${E_{1}, E_{2}, ..., E_{D}}$，并将每组部署在单个设备上。设备级平衡损失的计算如下：
$$
\mathcal{L}_{DevBal }=\alpha_{2} \sum_{i=1}^{D} f_{i}' P_{i}', \quad(26)\\
f_{i}'=\frac{1}{\left|\mathcal{E}_{i}\right|} \sum_{j \in \mathcal{E}_{i}} f_{j}, \quad(27)\\
P_{i}'=\sum_{j \in \mathcal{E}_{i}} P_{j}, \quad(28)
$$
其中，$\alpha_{2}$是一个称为设备级平衡因子的超参数。



通信平衡损失。最后，我们引入通信平衡损失以确保每个设备的通信达到平衡。尽管设备受限的路由机制保证了每个设备的发送通信量是有界的，但如果某个设备接收的令牌比其他设备多，实际通信效率也会受到影响。为了缓解这一问题，我们设计了如下通信平衡损失： 
$$
\mathcal{L}_{CommBal }=\alpha_{3} \sum_{i=1}^{D} f_{i}'' P_{i}'',\\
f_{i}''=\frac{D}{M T} \sum_{t=1}^{T} \mathbb{1}( Token t is sent to Device i), \\ 
P_{i}''=\sum_{j \in \mathcal{E}_{i}} P_{j}, (31)
$$
其中，$\alpha_{3}$是一个超参数，称为通信平衡因子。设备受限路由机制的工作原理是确保每个设备最多向其他设备传输$M T$个隐藏状态。同时，利用通信平衡损失促使每个设备从其他设备接收约$M T$个隐藏状态。通信平衡损失保证了设备间信息的均衡交换，有助于提高通信效率。

2.2.4. 令牌丢弃策略

虽然平衡损失旨在促进负载均衡，但需要承认的是，它们无法保证严格的负载平衡。为了进一步减少由负载不平衡导致的计算资源浪费，我们在训练过程中引入了一种设备级别的令牌丢弃策略。该方法首先计算每个设备的平均计算预算，这意味着每个设备的容量系数都等于1.0。然后，受Riquelme等人（2021）的启发，我们在每个设备上丢弃亲和力分数最低的令牌，直到达到计算预算为止。此外，我们确保约10%的训练序列所包含的令牌永远不会被丢弃。通过这种方式，我们可以根据效率需求灵活决定在推理过程中是否丢弃令牌，并始终保证训练和推理之间的一致性。

![img](/Users/wangqi/workspace/document/LLM-learn/images/deepseek-v2-pdf-08-24-2025_11_08_PM.png)

MLA(低秩键值联合压缩):

对比下MHA和MLA

令$d$为嵌入维度维度，$n_{h}$ 是注意力头数，$d_{h}$是每个头的维度，以及 $h_{t} \in \mathbb{R}^{d}$成为第$𝑡$个标记在注意力层的注意力输入。

MHA计算：

标准MHA首先通过三个矩$W^{Q}、W^{K}、W^{V} \in \mathbb{R}^{d_{h} n_{h} ×d}$分别生成$q_{t}，k_{t}， v_{t} \in \mathbb{R}^{d_{h} n_{h}}:$
$$
q_{t}=W^{Q} h_{t}, \quad(1)\\
k_{t}=W^{K} h_{t}, \quad(2)\\ 
v_{t}=W^{V} h_{t}, \quad(3)
$$
然后，$q_{t}、k_{t}、v_{t}$将被分割为$n_{h}$个头，用于多头注意力计算：
$$
\left[q_{t, 1}, q_{t, 2} ; ... ; q_{t, n_{h}}\right]=q_{t}, \quad(4) \\
\left[k_{t, 1} ; k_{t, 2} ; ... ; k_{t, n_{h}}\right]=k_{t}, \quad(5) \\ 
\left[v_{t, 1} ; v_{t, 2} ; ... ; v_{t, n_{h}}\right]=v_{t}, \quad(6) \\
o_{t, i}=\sum_{j=1}^{t} Softmax_{j}\left(\frac{q_{t, i}^{T} k_{j, i}}{\sqrt{d_{h}}}\right) v_{j, i}, \quad(7)\\
u_{t}=W^{O}\left[o_{t, 1} ; o_{t, 2} ; ... ; o_{t, n_{h}}\right], \quad(8)
$$
其中，$q_{t,i}、k_{t,i}、v_{t,ii} \in \mathbb{R}^{d_{k}}$分别表示第$i$个注意力头的查询、键和值；

MLA计算：
MLA的核心是对键和值进行低秩联合压缩以减少KV缓存：
$$
c_{t}^{K V}=W^{D K V} h_{t}, \quad(9) \\
k_{t}^{C}=W^{U K} c_{t}^{K V}, \quad(10) \\
v_{t}^{C}=W^{U V} c_{t}^{K V}, \quad(11) \\
$$
其中$c_{t}^{K V} \in \mathbb{R}^{d_{c}}$是$key$和$value$的压缩潜在向量；$d_{c}(\ll d_{h} n_{h})$表示KV压缩维度；$W^{D K V} \in \mathbb{R}^{d_{c} ×d}$ 是下投影矩阵；以及 $W^{UK}$、$W^{U V} \in \mathbb{R}^{d_{h} n_{h} ×d_{c}}$分别是$key$和$value$的上投影矩阵。在推理过程中，MLA只需缓存$c_{t}^{K V}$，因此其KV缓存仅有$d_{c} l$个元素，其中$𝑙$表示层数。此外，在推理过程中，由于$W^{U K}$可被融入$W^{Q}$，且$W^{U V}$可被融入$W^{O}$，甚至无需为注意力计算出$key$和$value$， $W^{O} \in \mathbb{R}^{d ×d_{h} n_{h}}$表示输出投影矩阵。

![img](/Users/wangqi/workspace/document/LLM-learn/images/deepseek-v2-pdf-08-24-2025_11_10_PM.png)

ROPE解耦

由于ROPE位置编码和MLA不兼容，提出了分离式ROPE方式，使用额外的多头查询$q_{t, i}^{R} \in \mathbb{R}^{d_{h}^{R}}$和一个共享$key$ $k_{t}^{R} \in \mathbb{R}^{d_{k}^{R}}$作用于RoPE，其中$d_{h}^{R}$表示每个头的解耦的$Query$和$Key$的维度。

MLA执行如下计算:
$$
c_{t}^{Q}=W^{D Q} h_{t}, \quad(12)\\  
\left[q_{t, 1}^{C} ; q_{t, 2}^{C} ; ... ; q_{t, n_{h}}^{C}\right]=q_{t}^{C}=W^{U Q} c_{t}^{Q}, \quad(13)\\
\left[q_{t, 1}^{R} ; q_{t, 2}^{R} ; ... ; q_{t, n_{h}}^{R}\right]=q_{t}^{R}=RoPE\left(W^{QR} c_{t}^{Q}\right), \quad(14) \\
q_{t, i}=\left[q_{t, i}^{C} ; q_{t, i}^{R}\right], \quad(15)\\
c_{t}^{K V}=W^{D K V} h_{t}, \quad(16)\\
\left[k_{t, 1}^{C} ; k_{t, 2}^{C} ; ... ; k_{t, n_{h}}^{C}\right]=k_{t}^{C}=W^{U K} c_{t}^{K V}, \quad(17)\\
k_{t}^{R}=RoPE\left(W^{K R} h_{t}\right), \quad(18)\\
k_{t, i}=\left[k_{t, i}^{C} ; k_{t}^{R}\right], \quad(19)\\
\left[v_{t, 1}^{C} ; v_{t, 2}^{C} ; ... ; v_{t, n_{h}}^{C}\right] = v_{t}^{C} = W^{UV}c_{t}^{KV}, \quad(20)\\
o_{t, i}=\sum_{j=1}^{t} Softmax_{j}\left(\frac{q_{t, i}^{T} k_{j, i}}{\sqrt{d_{h}+d_{h}^{R}}}\right) v_{j, i}^{C},\quad(21) \\
u_{t}=W^{O}\left[o_{t, 1} ; o_{t, 2} ; ... ; o_{t, n_{h}}\right], \quad(22)
$$

$\left[.;. \right]$表示拼接，$W^{Q R} \in R^{d_{h}^{R} n_{h} ×d_{c}'}$ 和 $W^{K R} \in R^{d_{h}^{R} ×d}$用于生成解耦$Query$和$Key$的矩阵。

其中$c_{t}^{KV}$和$k_{t}^{R}$需要缓存用于生成。在推理过程中，原始公式需要从$c_{t}^{K V}$中恢复出$k_{t}^{c}$和$v_{t}^{c}$以进行注意力计算。幸运的是，由于矩阵乘法的结合律，我们可以将$W^{U K}$合并到$W^UQ$中，将$W^{U V}$合并到$W^{O}$中。因此，我们无需为每个查询计算出键和值。通过这种优化，我们避免了在推理过程中重新计算$k_{r}^{C}$和$v_{t}^{C}$所带来的计算开销。

https://zhuanlan.zhihu.com/p/16730036197

https://spaces.ac.cn/archives/10091

https://zhuanlan.zhihu.com/p/1911795330434986569

上下文扩展：

在DeepSeek-V2的初始预训练之后，我们采用YaRN（Peng等人，2023）将默认的上下文窗口长度从4K扩展到128K。YaRN被专门应用于解耦的共享$key\ k_{t}^{R}$，因为它负责承载RoPE（Su等人，2024）。对于YaRN，我们将比例$s$设置为40，$\alpha$设置为1，$\beta$设置为32，目标最大上下文长度设置为160K。在这些设置下，我们可以预期模型在128K的上下文长度下会有良好的响应。由于我们独特的注意力机制，与原始YaRN略有不同，参数$\sqrt{t}$，计算方式为$\sqrt{t}=0.0707 \ln s+1$，目的是最小化困惑度。

**Llama 3**

Tokenization: 12.8K词表

Position-embedding: RoPE base=500000



**qwen2**

attention: 采用GQA注意力机制

位置编码：使用YARN调整注意力权重，实现更好的长度外推，RoPE base=1000000

模型结构：使用独立专家的MOE结构



**qwen2.5**

上下文扩展：为实现最佳训练效率，Qwen2.5采用两阶段预训练方法：初始阶段使用4096token的上下文长度，随后是针对更长序列的扩展阶段。遵循Qwen2中使用的策略，我们在所有模型变体（除Qwen2.5-Turbo外）的最终预训练阶段，将上下文长度从4096扩展至32768token。同时，我们利用ABF技术（Xiong等人，2023）将RoPE的基准频率从10000提高到1000000。为了增强我们的模型在推理过程中处理更长序列的能力，我们实施了两项关键策略：YARN（Peng等人，2023年）和双块注意力机制（DCA，An等人，2024年）。通过这些创新，我们将序列长度处理能力提升了四倍，使Qwen2.5-Turbo能够处理多达100万个令牌，其他模型能够处理多达131,072个令牌。值得注意的是，这些方法不仅通过降低困惑度改进了长序列的建模效果，还保持了模型在短序列上的出色表现，确保了在不同输入长度下的质量一致性。

模型结构：使用带有共享专家的MOE结构



**deepseek-v3**

2.1.2. 具有无辅助损失负载均衡的DeepSeekMoE

**DeepSeekMoE的基本架构**

对于前馈网络（FFN），DeepSeek-V3采用了DeepSeekMoE架构（Dai等人，2024）。与GShard（Lepikhin等人，2021）等传统MoE架构相比，DeepSeekMoE使用更细粒度的专家，并将部分专家隔离为共享专家。令$u_{t}$表示第t个标记的FFN输入，我们按如下方式计算FFN输出$h_{t}'$： 
$$
h_{t}'=u_{t}+\sum_{i=1}^{N_{s}} FFN_{i}^{(s)}\left(u_{t}\right)+\sum_{i=1}^{N_{r}} g_{i, t} FFN_{i}^{(r)}\left(u_{t}\right), \quad(12) \\
g_{i, t}=\frac{g_{i, t}'}{\sum_{j=1}^{N_{r}} g_{j, t}'}, \quad(13) \\
g_{i, t}'= \begin{cases}s_{i, t}, & s_{i, t} \in Topk\left(\left\{s_{j, t} | 1 \leq j \leq N_{r}\right\}, K_{r}\right), \\ 0, & otherwise, \end{cases} \quad(14) \\
s_{i, t}=Sigmoid\left(u_{t}^{T} e_{i}\right), \quad(15)
$$

其中，$N_{s}$和$N_{r}$分别表示共享专家和路由专家的数量；$FFN_{i}^{(s)}(\cdot)$和$FFN_{i}^{(r)}(\cdot)$分别表示第i个共享专家和第i个路由专家；K表示激活的路由专家数量；$g_{i, t}$ 是第𝑖个专家的选通值；$s_{i, t}$ 是token-to-expert 亲和力；$e_{i}$ 是第𝑖个路由专家的质心向量；Topk $(\cdot, K)$ 表示在为第𝑡个token和所有路由专家计算的亲和力分数中，包含K个最高分数的集合。与DeepSeek-V2略有不同，DeepSeek-V3使用sigmoid函数计算亲和力分数，并对所有选定的亲和力分数进行归一化以生成门控值。



**无辅助损失的负载均衡**

对于MoE模型而言，专家负载不均衡会导致路由崩溃（Shazeer等人，2017），并在专家并行的场景中降低计算效率。传统解决方案通常依靠辅助损失（Fedus等人，2021；Lepikhin等人，2021）来避免负载不均衡。然而，过大的辅助损失会损害模型性能（Wang等人，2024a）。为了在负载均衡和模型性能之间实现更好的权衡，我们开创了一种无辅助损失的负载均衡策略（Wang等人，2024a）来确保负载均衡。具体来说，我们为每个专家引入了一个偏置项$b_{i}$，并且将其添加到相应的亲和度分数$s_{i, t}$中，以确定前K路由：
$$
g_{i, t}'= \begin{cases}s_{i, t}, & s_{i, t}+b_{i} \in Topk\left(\left\{s_{j, t}+b_{j} | 1 \leq j \leq N_{r}\right\}, K_{r}\right), \\ 0, & otherwise. \end{cases}
$$

请注意，偏置项仅用于路由。将与FFN输出相乘的门控值仍源自原始亲和度分数$s_{i,t}$。在训练期间，我们保持监控每个训练步骤中整批数据的专家负载。在每个步骤结束时，如果相应的专家负载过重，我们会将偏置项减少$\gamma$；如果相应的专家负载不足，则将其增加$\gamma$，其中$\gamma$是一个称为偏置更新速度的超参数。通过这种动态调整，DeepSeek-V3在训练过程中保持了均衡的专家负载，并且比通过纯辅助损失来促进负载均衡的模型表现更好。



**互补的序列级辅助损失**

尽管DeepSeek-V3主要依靠无辅助损失策略来实现负载平衡，但为了防止任何单个序列内部出现极端不平衡，我们还采用了一种互补的序列级平衡损失： 
$$
\mathcal{L}_{Bal}=\alpha \sum_{i=1}^{N_{r}} f_{i} P_{i}, \quad(17)\\
f_{i}=\frac{N_{r}}{K_{r} T} \sum_{t=1}^{T} \mathbb{1}\left(s_{i, t} \in Topk\left(\left\{s_{j, t} | 1 \leq j \leq N_{r}\right\}, K_{r}\right)\right), \quad(18) \\
s_{i, t}'=\frac{s_{i, t}}{\sum_{j=1}^{N_{r}} s_{j, t}}, \quad(19) \\
P_{i}=\frac{1}{T} \sum_{t=1}^{T} s_{i, t'}' \quad(20)
$$
其中，平衡因子$\alpha$是一个超参数，在DeepSeek-V3中会被赋予一个极小的值；\mathbb{1}表示指示函数；$T$表示一个序列中的标记数量。序列级平衡损失有助于平衡每个序列上的专家负载。



MTP: 多token预测，受Gloeckle等人（2024）的启发，我们为DeepSeek-V3研究并设定了多 token 预测（MTP）目标，该目标将每个位置的预测范围扩展到多个未来 token。一方面，MTP 目标可增强训练信号，并可能提高数据效率。另一方面，MTP 或许能使模型预先规划其表征，从而更好地预测未来 token。图3展示了我们的 MTP 实现方式。与 Gloeckle 等人（2024）使用独立输出头并行预测 D 个额外 token 不同，我们会顺序预测额外 token，并在每个预测深度保留完整的因果链。我们将在本节介绍 MTP 实现的详细信息。



MTP模块。具体来说，我们的MTP实现使用D个顺序模块来预测D个额外的标记。第k个MTP模块由一个共享嵌入层Emb、一个共享输出头OutHead、一个Transformer块$TRM_{k}(\cdot)$和一个投影矩阵$M_{k} \in \mathbb{R}^{d ×2 d}$组成。对于第i个输入标记$t_{i}$，在第k个预测深度，我们首先将第k1个深度处第i个标记的表示$h_{i}^{k-1} \in \mathbb{R}^{d}$与第$(i+k)$个标记的嵌入$Emb(t_{i+k}) \in \mathbb{R}^{d}$相结合。通过线性投影：
$$
h_{i}^{\prime k}=M_{k}\left[RMSNorm\left(h_{i}^{k-1}\right) ; RMSNorm\left(Emb\left(t_{i+k}\right)\right)\right],
$$
其中$[.;.]$表示拼接。特别地，当$k=1$时，$h_{i}^{k-1}$指的是主模型给出的表示。注意，对于每个MTP模块，其嵌入层与主模型共享。组合后的$h_{i}^{\prime k}$作为第k层深度的Transformer块的输入，以生成当前深度$h_{i}^{k}$的输出表示:

$$
h_{1: T-k}^{k}=TRM_{k}\left(h_{1: T-k}^{\prime k}\right),  \quad(22)
$$

其中T表示输入序列长度，$i:j$表示切片操作（包含左右边界）。最后，以$h_{i}^{k}$作为输入，共享输出头将计算第k个附加预测标记$P_{i+1+k}^{k} \in \mathbb{R}^{V}$的概率分布，其中V是词汇量大小：
$$
P_{i+k+1}^{k}= OutHead\left(h_{i}^{k}\right) \quad(23)
$$

输出头$OutHead(·)$将表示线性映射为对数几率，随后应用$Softmax(.)$函数来计算第k个附加标记的预测概率。此外，对于每个MTP模块，其输出头与主模型共享。我们维持预测因果链的原则与EAGLE（Li等人，2024b）相似，但其主要目标是推测解码（Leviathan等人，2023；Xia等人，2023），而我们利用MTP来改进训练。

**MTP训练目标** 

对于每个预测深度，我们计算交叉熵损失$L_{MTP }^{k}$ :
$$
\mathcal{L}_{MTP }^{k}=CrossEntropy\left(P_{2+k: T+1}^{k}, t_{2+k: T+1}\right)=-\frac{1}{T} \sum_{i=2+k}^{T+1} log P_{i}^{k}\left[t_{i}\right],
$$
其中$T$表示输入序列长度，$t_{i}$表示第i个位置的真实标记，$P_{i}^{k}[t_{i}]$表示由第k个MTP模块给出的$t_{i}$的相应预测概率。最后，我们计算所有深度的MTP损失的平均值，并将其乘以权重因子λ，得到整体MTP损失$L_{MTP }$，这作为DeepSeek-V3的额外训练目标：
$$
\mathcal{L}_{MTP}=\frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{MTP}^{k} . (25)
$$

![img](/Users/wangqi/workspace/document/LLM-learn/images/deepseek-v3-pdf-08-26-2025_08_51_PM.png)

工程优化：FP8混合精度训练框架





**deepseek-r1**

> 在本文中，我们迈出了利用纯强化学习（RL）提升语言模型推理能力的第一步。我们的目标是探索大语言模型（LLMs）在没有任何监督数据的情况下发展推理能力的潜力，重点关注它们通过纯强化学习过程实现的自我进化。具体而言，我们以DeepSeek-V3-Base作为基础模型，并采用GRPO（Shao等人，2024）作为强化学习框架来提升模型的推理性能。在训练过程中，DeepSeek-R1-Zero自然展现出了许多强大且有趣的推理行为。经过数千步强化学习训练后，DeepSeek-R1-Zero在推理基准测试中表现卓越。例如，在2024年AIME测试中的pass@1得分从15.6%提升至71.0%，而通过多数投票，该得分进一步提高到86.7%，与OpenAI-o1-0912的性能相当。
>
> 然而，DeepSeek-R1-Zero面临着可读性差、语言混杂等挑战。为了解决这些问题并进一步提升推理性能，我们推出了DeepSeek-R1，它融入了少量冷启动数据和多阶段训练流程。具体来说，我们首先收集数千条冷启动数据来微调DeepSeek-V3-Base模型。在此之后，我们像DeepSeek-R1-Zero一样执行面向推理的强化学习（RL）。在强化学习过程接近收敛时，我们通过在强化学习检查点上进行拒绝采样来生成新的监督微调（SFT）数据，同时结合DeepSeek-V3在写作、事实问答和自我认知等领域的监督数据，然后重新训练DeepSeek-V3-Base模型。使用新数据完成微调后，该检查点会经历额外的强化学习过程，期间会考虑到所有场景的提示词。经过这些步骤，我们得到了一个名为DeepSeek-R1的检查点，其性能与OpenAI-o1-1217相当。我们进一步探索了从DeepSeek-R1到更小的密集型模型的蒸馏。使用Qwen2.5-32B（Qwen，2024b）作为基础模型，直接从DeepSeek-R1进行蒸馏的效果优于在其上应用强化学习。这表明，更大的基础模型所发现的推理模式对于提升推理能力至关重要。我们开源了蒸馏后的Qwen和Llama（Dubey等人，2024）系列模型。值得注意的是，我们蒸馏后的14B模型大幅优于最先进的开源模型QwQ-32B-Preview（Qwen，2024a），而蒸馏后的32B和70B模型在密集型模型的推理基准测试中创下了新纪录。

2.2.1. 强化学习算法-GRPO算法

**群体相对策略优化**

 为了节省强化学习的训练成本，我们采用了群体相对策略优化（GRPO）（Shao等人，2024），该方法摒弃了通常与策略模型大小相同的评论者模型，而是从群体分数中估计基线。具体来说，对于每个问题q，GRPO从旧策略\(\pi_{\theta_{old }}\)中采样一组输出\({o_{1}, o_{2}, \cdots, o_{G}}\)，然后通过最大化以下目标来优化策略模型:
$$
\begin{aligned} 
\mathcal{J}_{G R P O}(\theta) =\mathbb{E}\left[q \sim P(Q),\left\{o_{i}\right\}_{i=1}^{G} \sim \pi_{\theta_{old }}(O | q)\right] \quad(1)\\ 
\frac{1}{G} \sum_{i=1}^{G}\left(min \left(\frac{\pi_{\theta}\left(o_{i} | q\right)}{\pi_{\theta_{old }}\left(o_{i} | q\right)} A_{i}, clip\left(\frac{\pi_{\theta}\left(o_{i} | q\right)}{\pi_{\theta_{old }}\left(o_{i} | q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_{i}\right)-\beta \mathbb{D}_{K L}\left(\pi_{\theta}|| \pi_{r e f}\right)\right),   \quad(2) \\

\mathbb{D}_{K L}\left(\pi_{\theta} \| \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_{i} | q\right)}{\pi_{\theta}\left(o_{i} | q\right)}-log \frac{\pi_{r e f}\left(o_{i} | q\right)}{\pi_{\theta}\left(o_{i} | q\right)}-1, \quad(3)
\end{aligned}
$$
其中ε和β是超参数，\(A_{i}\)是优势函数，利用与每组内输出相对应的一组奖励\({r_{1}, r_{2}, ..., r_{G}}\)计算得出：
$$
A_{i}=\frac{r_{i}-mean\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)}{std\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)} . (3)
$$
```markdown
A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. 
User: prompt. 
Assistant:
```

DeepSeek-R1-Zero的模板。在训练过程中，**prompt**将被替换为具体的推理问题。

2.2.2. 奖励建模

奖励是训练信号的来源，它决定了强化学习的优化方向。为了训练DeepSeek-R1-Zero，我们采用了一种基于规则的奖励系统，该系统主要包含两种类型的奖励：

• 准确性奖励：准确性奖励模型用于评估响应是否正确。例如，对于具有确定性结果的数学问题，要求模型以指定格式（如在方框内）提供最终答案，以便通过可靠的基于规则的方式验证正确性。同样，对于LeetCode问题，可以使用编译器根据预定义的测试用例生成反馈。

格式奖励：除了准确性奖励模型外，我们还采用了一种格式奖励模型，该模型强制模型将其思考过程放在‘<think>’和‘</think>’标签之间。

在开发DeepSeek-R1-Zero时，我们没有应用结果型或过程型神经奖励模型，因为我们发现神经奖励模型在大规模强化学习过程中可能会遭遇奖励欺骗问题，而且重新训练奖励模型需要额外的训练资源，还会使整个训练流程变得复杂。



**Gemma 3**

模型结构：滑动窗口注意力旨在减少KV缓存的内存需求，不同层设置局部或者全局注意力机制，局部：全局=5:1交替进行， sliding window=1024

Normalization: 在注意力模块和前馈模块前后都放置了RMSNorm层。这种设计结合了前归一化和后归一化的优点，既保持了训练稳定性，又提高了推理效率。



**Mistral Small 3.1**



**Llama 4**

模型架构：MOE和密集模块交替执行



**qwen3**

引入思考-非思考模式

偏置:移除QKV偏置，引入QK-norm，提升稳定性

模型架构：独立专家的MOE架构，专家数128，激活专家8个



**SmolLM3**

位置编码：NoPE不使用任何位置嵌入（绝对位置嵌入或旋转位置嵌入），而是依赖因果注意力掩码来保持序列的自回归顺序。这种设计使得模型在训练过程中能够学习到隐式的位置信息。



**kimi k2**

Optimizer: 提出了MunonClip优化器，整合了Muon算法和QK-Clip稳定性增强机制。



**GLM-4.5**



**Gpt-oss**

为了降低内存占用，模型对占总参数量 90% 以上的 MoE 权重进行了 [MXFP4](https://zhida.zhihu.com/search?content_id=740722578&content_type=Answer&match_order=1&q=MXFP4&zhida_source=entity) 格式的后训练量化，将权重压缩至 4.25 位/参数。

- 对每个注意力头，设置一个可以学习的标量，然后进行softmax汇聚。
- 与GPT-3相同，交替使用滑动窗口层和全连接层。
- 对每个输入分配4个相关专家处理，再整合结果，专家之间彼此完全独立，同时使用标准负载均衡损失，确保资源高效分配。
- 使用了改进的swiglu激活函数，通过α=1.702让sigmoid的线性单元silu近似于高斯误差线性单元gelu。裁剪激活值防止梯度爆炸，通过调整“up+1”有助于梯度流动。
- 采用[YaRN上下文窗口扩展技术](https://zhida.zhihu.com/search?content_id=261401785&content_type=Article&match_order=1&q=YaRN上下文窗口扩展技术&zhida_source=entity)，提升长文本处理能力。
- 移除了RMSNorm归一化过程中的可学习偏置参数，减少拟合风险。
